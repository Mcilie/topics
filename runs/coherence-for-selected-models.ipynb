{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0724588c769277eec1559a818f4d33dcaba65e848ce2ee61833dc206a0b8113e6",
   "display_name": "Python 3.9.4 64-bit ('topic-evaluation': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from calculate_coherence import (\n",
    "    make_runs, gen_measure_name, SLURM_HEADER, save_json, save_text, load_json\n",
    ")"
   ]
  },
  {
   "source": [
    "## Calculate a bunch of coherence metrics for the selected models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"./results/full-mindf_power_law-maxdf_0.9\"\n",
    "output_dir = \"./outputs/full-mindf_power_law-maxdf_0.9\"\n",
    "coherence_measure = \"c_npmi_10_full\"\n",
    "overlapping_word_threshold = 5"
   ]
  },
  {
   "source": [
    "# selected with collect-runs.ipynb\n",
    "mallet = load_json(Path(results_dir, f\"mallet-topics-best-{coherence_measure}.json\"))\n",
    "dvae = load_json(Path(results_dir, f\"dvae-topics-best-{coherence_measure}.json\"))\n",
    "etm = load_json(Path(results_dir, f\"etm-topics-best-{coherence_measure}.json\"))\n",
    "\n",
    "model_results = {\"mallet\": mallet, \"dvae\": dvae, \"etm\": etm}\n",
    "\n",
    "class DummyArgs:\n",
    "    input_dir = None\n",
    "    start_at = None\n",
    "    eval_every_n = None\n",
    "    eval_last_only = True\n",
    "    coherence_measure = None\n",
    "    reference_corpus = None\n",
    "    top_n = 10\n",
    "    window_size = None\n",
    "    python_path = \"/workspace/.conda/envs/gensim/bin/python\"\n",
    "    update_existing = False\n",
    "\n",
    "args = DummyArgs()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_commands(references, metrics, model_results):\n",
    "    commands = []\n",
    "    for ref, metric in itertools.product(references, metrics):\n",
    "        for model in model_results:\n",
    "            for data in model_results[model]:\n",
    "                args.coherence_measure = metric\n",
    "                args.input_dir = model_results[model][data][\"path\"]\n",
    "                args.reference_corpus = \"full\" if data in ref else ref\n",
    "                args.window_size = 10 if metric == \"c_npmi\" else None\n",
    "                command = make_runs(args, save=False)\n",
    "                if command:\n",
    "                    commands += command\n",
    "    return commands\n",
    "\n",
    "npmis_and_c_v = gen_commands([\"wikitext_full\", \"nytimes_full\", \"train\", \"val\", \"test\"], [\"c_npmi\", \"c_v\"], model_results)\n",
    "other_metrics = gen_commands([\"full\"], [\"u_mass\", \"c_uci\"], model_results)\n",
    "commands = npmis_and_c_v + other_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "found 55 runs\n"
     ]
    }
   ],
   "source": [
    "slurm_log_dir = Path(output_dir, \"_run-logs/coherence/slurm-logs\")\n",
    "slurm_header = SLURM_HEADER.format(n_jobs=len(commands)-1, log_dir=slurm_log_dir)\n",
    "commands = [slurm_header] + [\n",
    "    f\"test ${{SLURM_ARRAY_TASK_ID}} -eq {run_id} && sleep {run_id}s && {run_command}\"\n",
    "    for run_id, run_command in enumerate(commands)\n",
    "]\n",
    "slurm_sbatch_script = \"\\n\".join(commands)\n",
    "print(f\"found {len(commands)} runs\")\n",
    "save_text(slurm_sbatch_script, \"./coherence-best-model-runs.sh\")"
   ]
  },
  {
   "source": [
    "## Collect results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"wikitext\", \"nytimes\"]\n",
    "coherences = {}\n",
    "for data in datasets:\n",
    "    coherences[data] = {}\n",
    "    for model in model_results:\n",
    "        coherences[data][model] = {}\n",
    "        coherences[data][model][\"metrics\"] = {}\n",
    "        coherences[data][model][\"topics\"] = [t[:20] for t in model_results[model][data][\"topics\"]]\n",
    "\n",
    "        path = model_results[model][data][\"path\"]\n",
    "        coh_data = load_json(Path(path, \"coherences.json\"))\n",
    "        for metric in sorted(coh_data):\n",
    "            coh_by_epoch = list(coh_data[metric].values())\n",
    "            assert(len(coh_by_epoch) == 1)\n",
    "            final_coh = coh_by_epoch[-1][\"by_topic\"]\n",
    "            coherences[data][model][\"metrics\"][metric] = final_coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(coherences, Path(results_dir, \"coherences-for-selected-models.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['c_npmi_10_full',\n",
       " 'c_npmi_10_test',\n",
       " 'c_npmi_10_train',\n",
       " 'c_npmi_10_val',\n",
       " 'c_npmi_10_wikitext_full',\n",
       " 'c_uci_full',\n",
       " 'c_v_full',\n",
       " 'c_v_test',\n",
       " 'c_v_train',\n",
       " 'c_v_val',\n",
       " 'c_v_wikitext_full',\n",
       " 'u_mass_full']"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "sorted(coh_data.keys())"
   ]
  },
  {
   "source": [
    "'c_npmi_10_full',cool\n",
    "'c_npmi_10_test',cool\n",
    "'c_npmi_10_train',cool\n",
    "'c_npmi_10_val',cool\n",
    "'c_npmi_10_wikitext_full',cool\n",
    "'c_uci_full',cool\n",
    "'c_v_full',cool\n",
    "'c_v_test',cool\n",
    "'c_v_train',cool\n",
    "'c_v_val',cool\n",
    "'c_v_wikitext_full',cool\n",
    "'u_mass_full'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* c_npmi_10_full' : NPMI with 10-word window on full (train+val+test) data, about ~1.82m docs for NYT, ~4.65m for wiki, following Lau 2014. This is used for model selection.\n",
    "* c_npmi_10_test: NPMI with 10-word window on test data, about . Basically here for completeness since the sizes are very similar to \"full\"\n",
    "* c_npmi_10_train: NPMI with 10-word window on train data. 28k docs for wiki, \n",
    "* c_npmi_10_val: 4200 for wiki\n",
    "* c_npmi_10_wikitext/nytimes_full:\n",
    "* c_uci_full:\n",
    "* c_v_full:\n",
    "* c_v_test:\n",
    "* c_v_train:\n",
    "* c_v_val:\n",
    "* c_v_wikitext_full:\n",
    "* u_mass_full: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}