{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('statsmodels-dev': conda)"
  },
  "interpreter": {
   "hash": "cf30b2a248a022118eaf3990843f204490a3a4dad78b3cc35fb5102bf723d6fa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest, proportions_ztost, test_proportions_2indep\n",
    "from statsmodels.stats.nonparametric import rank_compare_2indep\n",
    "from statsmodels.stats.weightstats import ttest_ind, ttost_ind\n",
    "from scipy.stats import pearsonr, spearmanr, mannwhitneyu, ttest_rel"
   ]
  },
  {
   "source": [
    "## Power Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bin_array(topics, questions=30):\n",
    "    selections = np.zeros(topics)\n",
    "    selections[:questions]  = 1\n",
    "    np.random.shuffle(selections)\n",
    "    return selections\n",
    "\n",
    "def permutation_test(a, b, alternative=\"two-sided\", value=0, iters=1000):\n",
    "    og_diff = np.mean(a) - np.mean(b) - value\n",
    "    combin = np.concatenate([a, b])\n",
    "    n_a, n_b = len(a), len(b)\n",
    "    diffs = [\n",
    "        np.mean(samp[:n_a]) - np.mean(samp[n_a:])\n",
    "        for _ in range(iters)\n",
    "        for samp in [np.random.choice(combin, n_a + n_b, replace=False)]\n",
    "    ]\n",
    "    if alternative == \"two-sided\":\n",
    "        return og_diff, np.mean(np.abs(og_diff) < np.abs(diffs)), diffs\n",
    "    elif alternative == \"larger\":\n",
    "        return og_diff, np.mean(og_diff < diffs), diffs\n",
    "    elif alternative == \"smaller\":\n",
    "        return og_diff, np.mean(og_diff > diffs), diffs\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be one of (two-sided, larger, smaller)\")"
   ]
  },
  {
   "source": [
    "### intrusion power analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "topics = 50\n",
    "annotators_per_topic = 26\n",
    "prob_correct = np.array([\n",
    "    1/6, # how often someone selects the intruder for an incoherent topic\n",
    "    0.85, # how often someone selects the intruder for a coherent topic. estimated from chang 2009: avg. % correct of the 10% most-correct topics\n",
    "])\n",
    "bad_topic_difference = 4 # effect size (number of bad topics in model a vs model b)\n",
    "h0 = 0.0\n",
    "\n",
    "# simulation\n",
    "iter = 10_000\n",
    "pvals = []\n",
    "alternative = \"larger\" if h0 == 0 else \"smaller\"\n",
    "np.random.seed(454)\n",
    "\n",
    "for i in range(iter):\n",
    "    # create topics for both models\n",
    "    model_a_topics = np.random.choice([0, 1], topics)\n",
    "    if model_a_topics.sum() - bad_topic_difference < 0:\n",
    "        continue\n",
    "    model_b_topics = np.zeros(topics, dtype=int)\n",
    "    model_b_topics[:int(model_a_topics.sum() - bad_topic_difference)] = 1\n",
    "\n",
    "    # simulate responses\n",
    "    n = topics*annotators_per_topic\n",
    "    topic_assignments = annotators_per_topic\n",
    "    prob_a = prob_correct[model_a_topics]\n",
    "    model_a_responses = np.random.binomial(n=topic_assignments, p=prob_a)\n",
    "    prob_b = prob_correct[model_b_topics]\n",
    "    model_b_responses = np.random.binomial(n=topic_assignments, p=prob_b)\n",
    "    \n",
    "    # do test\n",
    "    #pval, _, _ = proportions_ztost([model_a_responses.sum(), model_b_responses.sum()], [n, n], low=-5, upp=h0)\n",
    "    stat, pval = proportions_ztest([model_a_responses.sum(), model_b_responses.sum()], [n, n], value=h0, alternative=alternative)\n",
    "    #stat, pval = test_proportions_2indep(model_a_responses.sum(), n, model_b_responses.sum(), n, alternative='larger', compare='diff')\n",
    "    pvals.append(pval)\n",
    "pvals = np.array(pvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "alpha  0.01, power: 0.737\nalpha 0.025, power: 0.877\nalpha  0.05, power: 0.946\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.01, 0.025, 0.05]:\n",
    "    print(f\"alpha {alpha:5}, power: {np.mean(pvals < alpha):0.3f}\")"
   ]
  },
  {
   "source": [
    "### ratings power analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 311.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "topics = 50\n",
    "annotators_per_topic = 15\n",
    "# probabilities of selecting bad, okay, good when \n",
    "# the \"true\" topic is bad, okay, or good\n",
    "topic_probs = np.array([\n",
    "    [0.75, 0.25, 0.0], # bad topic probs\n",
    "    [0.25, 0.5, 0.25], # okay topic probs\n",
    "    [0.0, 0.25, 0.75], # good topic probs\n",
    "])\n",
    "bad_topic_difference = 4 # effect size (number of bad topics in model a vs model b)\n",
    "h0 = 0.0 # null value. Set to >0 if `bad_topic_difference` is 0\n",
    "\n",
    "# simulation\n",
    "iter = 10_000\n",
    "pvals_rho, pvals_t, pvals_u = [], [], []\n",
    "alternative = \"larger\" if h0 == 0 else \"smaller\"\n",
    "\n",
    "for i in tqdm(range(iter), total=iter):\n",
    "    # create topics for both models\n",
    "    model_a_topics = np.random.choice([0, 1, 2], topics)\n",
    "    good_topics_in_a = (model_a_topics == 2).sum()\n",
    "    okay_topics_in_a = (model_a_topics == 1).sum()\n",
    "    good_topics_in_b = good_topics_in_a - bad_topic_difference\n",
    "    if good_topics_in_b < 0:\n",
    "        continue\n",
    "    model_b_topics = np.zeros(topics, dtype=int)\n",
    "    model_b_topics[:good_topics_in_b] = 2\n",
    "    model_b_topics[good_topics_in_b:okay_topics_in_a+good_topics_in_b] = 1\n",
    "    assert((model_b_topics == 1).sum() == okay_topics_in_a)\n",
    "\n",
    "    # simulate responses\n",
    "    n = topics*annotators_per_topic\n",
    "\n",
    "    prob_a = topic_probs[model_a_topics]\n",
    "    model_a_responses = np.array([\n",
    "        np.random.multinomial(annotators_per_topic, p) for p in prob_a\n",
    "    ])\n",
    "    model_a_responses = np.repeat([0, 1, 2], model_a_responses.sum(0))\n",
    "    prob_b = topic_probs[model_b_topics]\n",
    "    model_b_responses = np.array([\n",
    "        np.random.multinomial(annotators_per_topic, p) for p in prob_b\n",
    "    ])\n",
    "    model_b_responses = np.repeat([0, 1, 2], model_b_responses.sum(0))\n",
    "    # do test\n",
    "    # t-test\n",
    "    stat, pval, dof = ttest_ind(\n",
    "        model_a_responses,\n",
    "        model_b_responses,\n",
    "        value=h0,\n",
    "        alternative=alternative,\n",
    "    )\n",
    "    pvals_t.append(pval)\n",
    "    # https://www.uvm.edu/~statdhtx/StatPages/More_Stuff/OrdinalChisq/OrdinalChiSq.html\n",
    "    # \"ordinal\" chi-sq\n",
    "    if h0 == 0:\n",
    "        stat, pval = pearsonr(\n",
    "            np.concatenate([model_b_responses, model_a_responses]),\n",
    "            np.concatenate([np.zeros(n), np.ones(n)])\n",
    "        )\n",
    "        pvals_rho.append(pval)\n",
    "    # mann-whitney\n",
    "    if h0 == 0:\n",
    "        stat, pval = mannwhitneyu(\n",
    "            model_a_responses,\n",
    "            model_b_responses,\n",
    "            alternative=\"greater\",\n",
    "        )\n",
    "        pvals_u.append(pval)\n",
    "pvals_rho = np.array(pvals_rho)\n",
    "pvals_t = np.array(pvals_t)\n",
    "pvals_u = np.array(pvals_u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "alpha: 0.01\n",
      "    power for corr.: nan, power for ttest: 0.753, power for m-w: nan\n",
      "alpha: 0.025\n",
      "    power for corr.: nan, power for ttest: 0.891, power for m-w: nan\n",
      "alpha: 0.05\n",
      "    power for corr.: nan, power for ttest: 0.961, power for m-w: nan\n",
      "/workspace/.conda/envs/topic-preprocessing/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/workspace/.conda/envs/topic-preprocessing/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.01, 0.025, 0.05]:\n",
    "    print(\n",
    "        f\"alpha: {alpha}\\n\",\n",
    "        f\"   power for corr.: {np.mean(pvals_rho < alpha):0.3f},\",\n",
    "        f\"power for ttest: {np.mean(pvals_t < alpha):0.3f},\",\n",
    "        f\"power for m-w: {np.mean(pvals_u < alpha):0.3f}\",\n",
    "    )"
   ]
  },
  {
   "source": [
    "### auto power analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 3316.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "topics = 50\n",
    "mean = 0.5\n",
    "# npmi\n",
    "sd_gamma_params = 15, 0.005 # roughly lines up with empirical\n",
    "# c_v\n",
    "#sd_gamma_params = 20, 0.005\n",
    "\n",
    "bad_topic_difference = 7 # effect size (number of bad topics in model a vs model b)\n",
    "h0 = 0.0\n",
    "\n",
    "# simulation\n",
    "iter = 10_000\n",
    "pvals = []\n",
    "alternative = \"larger\" if h0 == 0 else \"smaller\"\n",
    "\n",
    "for i in tqdm(range(iter), total=iter):\n",
    "    # create topics for both models\n",
    "    sd = max(0.0001, np.random.gamma(*sd_gamma_params))\n",
    "    model_a_scores = np.random.normal(mean, sd, size=topics)\n",
    "    # model b is like model a but...\n",
    "    model_b_scores = np.sort(np.random.normal(mean, sd, size=topics))\n",
    "    # the highest-scoring topics are replaced with the lowest scoring, plus some noise\n",
    "    if bad_topic_difference > 0:\n",
    "      model_b_scores[topics - bad_topic_difference:] = np.random.normal(model_b_scores[0], sd, size=bad_topic_difference)\n",
    "    \n",
    "    # do test\n",
    "    #stat, pval = rank_compare_2indep(model_a_scores, model_b_scores)\n",
    "   # stat, pval, diffs = permutation_test(model_a_scores, model_b_scores, value=h0, alternative=alternative, iters=500)\n",
    "    stat, pval, dof = ttest_ind(model_a_scores, model_b_scores, value=h0, alternative=alternative)\n",
    "    # stat, pval = mannwhitneyu(model_a_scores, model_b_scores, alternative='greater')\n",
    "    pvals.append(pval)\n",
    "pvals = np.array(pvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "alpha  0.01, power: 0.578\nalpha 0.025, power: 0.717\nalpha  0.05, power: 0.819\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.01, 0.025, 0.05]:\n",
    "    print(f\"alpha {alpha:5}, power: {np.mean(pvals < alpha):0.3f}\")"
   ]
  }
 ]
}