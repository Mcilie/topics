{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "statsmodels-dev",
   "display_name": "statsmodels-dev",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, mannwhitneyu, ttest_ind"
   ]
  },
  {
   "source": [
    "# Setup\n",
    "## Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/human/all_data\"\n",
    "DATA_FILE = \"all_data.csv\"\n",
    "\n",
    "FILTER_ON_FAMILIARITY = True\n",
    "familiar = \"_familiar\" if FILTER_ON_FAMILIARITY else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_metric_names = [\n",
    " 'c_npmi_10_full',\n",
    " 'c_npmi_10_nytimes_full',\n",
    " #'c_npmi_10_test',\n",
    " 'c_npmi_10_train',\n",
    " 'c_npmi_10_val',\n",
    " 'c_npmi_10_wikitext_full',\n",
    " #'c_uci_full',\n",
    " 'c_v_full',\n",
    " 'c_v_nytimes_full',\n",
    " #'c_v_test',\n",
    " 'c_v_train',\n",
    " 'c_v_val',\n",
    " 'c_v_wikitext_full',\n",
    " #'u_mass_full'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data = pd.read_csv(Path(DATA_DIR, DATA_FILE))\n",
    "\n",
    "# complete the out-of-sample columns\n",
    "for dataset in [\"wikitext\", \"nytimes\"]:\n",
    "    for metric in [\"c_npmi_10\", \"c_v\"]:\n",
    "        task_data[f\"{metric}_{dataset}_full\"] = task_data[f\"{metric}_{dataset}_full\"].combine_first(task_data[f\"{metric}_full\"])\n",
    "\n",
    "# add the constant for lin. reg\n",
    "task_data = sm.add_constant(task_data)\n",
    "\n",
    "# convert infinite values to nans\n",
    "task_data = task_data.replace(np.inf, np.nan)\n"
   ]
  },
  {
   "source": [
    "## Helper functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_values(values, lbs, ubs):\n",
    "    \"\"\"\n",
    "    Given arrays of `values` and associated lower (`lbs`) and upper bounds (`ubs`),\n",
    "    for confidence intervals, return binary array of indices of `values` that have \n",
    "    CIs that overlap with the CI of the maximum value.\n",
    "    \"\"\"\n",
    "    n = len(values)\n",
    "    values = np.nan_to_num(values, nan=0)\n",
    "    lbs = np.nan_to_num(lbs, nan=0)\n",
    "    ubs = np.nan_to_num(ubs, nan=0)\n",
    "    sorted_value_idx = np.argsort(values)[::-1]\n",
    "    max_lbs = lbs[sorted_value_idx[0]]\n",
    "    for i, idx in enumerate(sorted_value_idx[1:], start=1):\n",
    "        # is the upper bound of this value contained in the\n",
    "        # lower bound of the largest coefficient?\n",
    "        if ubs[idx] < max_lbs:\n",
    "            return np.array([j in sorted_value_idx[:i] for j in range(n)])\n",
    "    return np.full(n, True)\n",
    "\n",
    "def find_min_values(values, lbs, ubs):\n",
    "    \"\"\"\n",
    "    Analogous to `find_max_values`\n",
    "    \"\"\"\n",
    "    return find_max_values(-values, -ubs, -lbs, contiguous=contiguous)\n",
    "\n",
    "def ci_lb(x):\n",
    "    return np.quantile(x, 0.025)\n",
    "\n",
    "def ci_ub(x):\n",
    "    return np.quantile(x, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEX_CLEANUP = {\n",
    "    \"c_v\": r\"$C_v$ (110-token window)\",\n",
    "    \"test\": \"Test\",\n",
    "    \"c_npmi_10\": r\"\\abr{npmi} (10-token window)\",\n",
    "    \"nytimes_full\": r\"\\abr{nyt}\",\n",
    "    \"wikitext_full\": r\"\\abr{wiki}\",\n",
    "    \"wikitext\": r\"\\abr{wiki}\",\n",
    "    \"nytimes\": r\"\\abr{nyt}\",\n",
    "    \"all\": \"Concatenated\",\n",
    "    \"full\": \"Full\",\n",
    "    \"train\": \"Train\",\n",
    "    \"val\": \"Val\",\n",
    "    \"test\": \"Test\",\n",
    "    \"ratings\": \"Rating\",\n",
    "    \"intrusions\": \"Intrusion\",\n",
    "    \"metric\": \"\",\n",
    "    \"dataset\": r\"Train Corpus $\\downarrow$\",\n",
    "    \"reference\": r\"Ref. Corpus $\\rightarrow$\",\n",
    "    \"task\": \"\",\n",
    "}"
   ]
  },
  {
   "source": [
    "# Bootstrapped correlations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 500/500 [10:57<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.05\n",
    "N_ITERS = 500\n",
    "NUM_ANNOTATORS = {\"intrusions\": 26, \"ratings\": 15}\n",
    "DROP_NA = False\n",
    "\n",
    "np.random.seed(42)\n",
    "rows = []\n",
    "for iteration in tqdm(range(N_ITERS), total=N_ITERS):\n",
    "    for task in [\"ratings\", \"intrusions\"]:\n",
    "        for dataset in [\"wikitext\", \"nytimes\", \"all\"]:\n",
    "            data_df = task_data.copy().loc[task_data.task == task]\n",
    "            if dataset != \"all\":\n",
    "                data_df = data_df.loc[data_df.dataset == dataset]\n",
    "            if FILTER_ON_FAMILIARITY:\n",
    "                data_df = data_df.loc[data_df.confidences_raw == 1]\n",
    "            \n",
    "            # sample \n",
    "            data_df = (\n",
    "                data_df.groupby([\"task\", \"dataset\", \"model\", \"topic_idx\"])\n",
    "                    .sample(NUM_ANNOTATORS[task], replace=True)\n",
    "                    .groupby([\"task\", \"dataset\", \"model\", \"topic_idx\"])[auto_metric_names + [\"scores_raw\"]]\n",
    "                    .mean()\n",
    "            )\n",
    "\n",
    "            # run correlations for each metric\n",
    "            for i, metric in enumerate(auto_metric_names):\n",
    "                if dataset in metric:\n",
    "                    continue # don't re-do the internal\n",
    "                if DROP_NA:\n",
    "                    df = data_df.dropna(subset=[metric])\n",
    "                else:\n",
    "                    df = data_df.fillna(0)\n",
    "                spear_rho, spear_p = spearmanr(df[metric].values, df[\"scores_raw\"].values)\n",
    "                pear_rho, pear_p = pearsonr(df[metric].values, df[\"scores_raw\"].values)\n",
    "                metric_base = re.search(\"c_npmi_10|c_v|c_uci|u_mass\", metric).group(0)\n",
    "                row = {\n",
    "                    \"task\": task,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"metric\": metric_base,\n",
    "                    \"reference\": metric.replace(f\"{metric_base}_\", \"\"),\n",
    "                    \"spear_rho\": spear_rho,\n",
    "                    \"pear_rho\": pear_rho,\n",
    "                    \"spear_p\": spear_p,\n",
    "                    \"pear_p\": pear_p,\n",
    "                }\n",
    "                rows.append(row)\n",
    "correlations = pd.DataFrame(rows)\n",
    "correlations.to_csv(f\"correlation_results{familiar}.csv\", index=False)"
   ]
  },
  {
   "source": [
    "## Create table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = pd.read_csv(f\"correlation_results{familiar}.csv\")\n",
    "correlations.loc[(correlations.dataset==\"wikitext\") & (correlations.reference == \"full\"), \"reference\"] = \"wikitext_full\"\n",
    "correlations.loc[(correlations.dataset==\"nytimes\") & (correlations.reference == \"full\"), \"reference\"] = \"nytimes_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_COL = \"spear_rho\"\n",
    "METRICS_TO_KEEP = [\"c_npmi_10\", \"c_v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_grouped = (\n",
    "    correlations.groupby([\"task\", \"dataset\", \"metric\", \"reference\"])[VALUE_COL]\n",
    "               .agg([\"mean\", \"std\", ci_lb, ci_ub])\n",
    "               .reset_index()\n",
    "               .rename(columns={\"mean\": VALUE_COL, \"ci_lb\": \"ci_0.025\", \"ci_ub\": \"ci_0.975\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_pivot = (\n",
    "    correlations_grouped.loc[correlations_grouped.metric.isin(METRICS_TO_KEEP)]\n",
    "               .sort_values([\"task\", \"dataset\", \"metric\", \"reference\"])\n",
    "               .pivot(index=[\"task\", \"dataset\"], columns=[\"metric\", \"reference\"], values=[VALUE_COL, \"ci_0.025\",  \"ci_0.975\"])\n",
    ")\n",
    "correlations_pivot = correlations_pivot[[\n",
    "    (v, m, r)\n",
    "    for v in [VALUE_COL, \"ci_0.025\",  \"ci_0.975\"]\n",
    "    for m in METRICS_TO_KEEP\n",
    "    for r in [\"nytimes_full\", \"wikitext_full\", \"train\", \"val\"]\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in correlations_pivot.iterrows():\n",
    "    # determine overlapping CIs\n",
    "    max_values = find_max_values(row[VALUE_COL], row[\"ci_0.025\"], row[\"ci_0.975\"])\n",
    "    newrow = []\n",
    "    # format each item in the row,\n",
    "    # bold for max value, underline for overlapping\n",
    "    for i, x in enumerate(row[VALUE_COL]):\n",
    "        if np.isnan(x):\n",
    "            val = \"-\"\n",
    "        elif not np.isnan(x) and max_values[i]:\n",
    "            val = r\"\\uline{\" + f\"{x:0.2f}\" + \"}\"\n",
    "            if x == np.max(row[VALUE_COL]):\n",
    "                val = r\"\\textbf{\" + val + \"}\"\n",
    "        else:\n",
    "            val = f\"{x:0.2f}\"\n",
    "        newrow.append(val)\n",
    "    correlations_pivot.at[idx, VALUE_COL] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\begin{tabular}{ll|rrrr|rrrr}\n\\toprule\n        &  & \\multicolumn{4}{c}{\\abr{npmi} (10-token window)} & \\multicolumn{4}{c}{$C_v$ (110-token window)} \\\\\n        & Ref. Corpus $\\rightarrow$ &  \\abr{nyt} &          \\abr{wiki} &         Train &   Val &  \\abr{nyt} &          \\abr{wiki} &         Train &   Val \\\\\n & Train Corpus $\\downarrow$ &               &                        &               &       &               &                        &               &       \\\\\n\\midrule\nIntrusion & \\abr{nyt} &          0.34 &           \\uline{0.51} &          0.32 &  0.25 &          0.44 &  \\textbf{\\uline{0.55}} &          0.42 &  0.38 \\\\\n        & \\abr{wiki} &  \\uline{0.39} &           \\uline{0.40} &  \\uline{0.40} &  0.14 &  \\uline{0.39} &  \\textbf{\\uline{0.40}} &  \\uline{0.39} &  0.13 \\\\\n        & Concatenated &          0.36 &           \\uline{0.45} &          0.35 &  0.18 &          0.41 &  \\textbf{\\uline{0.48}} &          0.41 &  0.26 \\\\\nRating & \\abr{nyt} &          0.45 &  \\textbf{\\uline{0.58}} &          0.44 &  0.43 &          0.51 &           \\uline{0.58} &  \\uline{0.53} &  0.52 \\\\\n        & \\abr{wiki} &  \\uline{0.45} &  \\textbf{\\uline{0.52}} &  \\uline{0.51} &  0.21 &  \\uline{0.44} &           \\uline{0.51} &  \\uline{0.51} &  0.23 \\\\\n        & Concatenated &          0.47 &  \\textbf{\\uline{0.54}} &          0.48 &  0.35 &  \\uline{0.49} &           \\uline{0.53} &  \\uline{0.51} &  0.42 \\\\\n\\bottomrule\n\\end{tabular}\n\n"
     ]
    }
   ],
   "source": [
    "# make latex\n",
    "correlations_pivot_values = correlations_pivot[VALUE_COL].loc[[\n",
    "    ('intrusions',  'nytimes'),\n",
    "    ('intrusions', 'wikitext'),\n",
    "    ('intrusions',      'all'),\n",
    "    (   'ratings',  'nytimes'),\n",
    "    (   'ratings', 'wikitext'),\n",
    "    (   'ratings',      'all'),\n",
    "]]\n",
    "latex = correlations_pivot_values.to_latex(escape=False, multicolumn_format='c', column_format=\"ll|rrrr|rrrr\")\n",
    "for to_replace, val in LATEX_CLEANUP.items():\n",
    "    latex = latex.replace(to_replace, val)\n",
    "print(latex)\n"
   ]
  },
  {
   "source": [
    "# Simple Simulation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_topics(df, num_topics=50):\n",
    "    \"\"\"\n",
    "    Sample `num_topics` random topics, with replacement, for each \n",
    "    (task, dataset, model) tuple\n",
    "    \"\"\"\n",
    "    topic_sample = (\n",
    "        df.groupby([\"task\", \"dataset\", \"model\", \"topic_idx\"])\n",
    "                    .size()\n",
    "                    .reset_index()\n",
    "                    .groupby([\"task\", \"dataset\", \"model\"])\n",
    "                    .sample(num_topics, replace=True)\n",
    "                    .drop(columns=0)\n",
    "    )\n",
    "    topic_sample[\"rand_topic_idx\"] = topic_sample.groupby(['task', 'dataset', 'model']).cumcount()\n",
    "    df = df.merge(topic_sample, how='inner')\n",
    "    return df\n",
    "\n",
    "def intrusion_test(scores_a, scores_b, alternative=\"larger\"):\n",
    "    \"\"\"\n",
    "    Proportions test of difference in intrusion scores\n",
    "    \"\"\"\n",
    "    return proportions_ztest(\n",
    "        [scores_a.sum(), scores_b.sum()],\n",
    "        [len(scores_a), len(scores_b)],\n",
    "        alternative=alternative\n",
    "    )\n",
    "\n",
    "def ratings_test(scores_a, scores_b, alternative=\"greater\"):\n",
    "    \"\"\"\n",
    "    Mann-Whitney U-test of difference in ratings cores\n",
    "    \"\"\"\n",
    "    return mannwhitneyu(scores_a, scores_b, alternative=alternative)\n",
    "\n",
    "def auto_test(scores_a, scores_b, alternative=\"greater\"):\n",
    "    \"\"\"\n",
    "    t-test of difference in automated scores\n",
    "    \"\"\"\n",
    "    return ttest_ind(scores_a, scores_b, equal_var=False, alternative=alternative)\n",
    "\n",
    "def false_discovery_rate_sim(\n",
    "    task,\n",
    "    test_df,\n",
    "    metric,\n",
    "    alpha=0.05,\n",
    "    beta=0.1,\n",
    "    models=[\"mallet\", \"dvae\", \"etm\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a sample of data, determine the number of times\n",
    "    that auto metrics imply a significant difference between models\n",
    "    when the human scores do not.\n",
    "    \"\"\"\n",
    "    false_positives, false_negatives = 0, 0\n",
    "    discoveries, omissions = 0, 0\n",
    "    if task == \"intrusions\":\n",
    "        stat_test = intrusion_test\n",
    "    elif task == \"ratings\":\n",
    "        stat_test = ratings_test\n",
    "    # model scores are the same across all the human ratings\n",
    "    model_scores = test_df.groupby(['task', 'dataset', 'model', 'rand_topic_idx']).head(1)\n",
    "\n",
    "    # Calculate the false discovery rate\n",
    "    for model_a, model_b in itertools.permutations(models, 2):\n",
    "        model_a_idxr = test_df.model==model_a\n",
    "        model_b_idxr = test_df.model==model_b\n",
    "        # run the tests for both human and auto metrics\n",
    "        stat_auto, p_auto = auto_test(\n",
    "            model_scores.loc[model_scores.model==model_a][metric],\n",
    "            model_scores.loc[model_scores.model==model_b][metric],\n",
    "        )\n",
    "        stat_human, p_human = stat_test(\n",
    "            test_df.loc[test_df.model==model_a][\"scores_raw\"],\n",
    "            test_df.loc[test_df.model==model_b][\"scores_raw\"],\n",
    "        )\n",
    "\n",
    "        # Count the disagreements while controlling for baseline\n",
    "        # probabilities of type I and II errors\n",
    "        # if p_auto < alpha: # rejects the null based on auto scores\n",
    "        #     if np.random.random() < alpha:\n",
    "        #         continue # falsely rejected the null, type I error\n",
    "        #     discoveries += 1\n",
    "        #     if np.random.random() < beta: # failed to detect effect for human. type II error\n",
    "        #         continue\n",
    "        #     false_positives += p_human > alpha # fail to reject the null for auto\n",
    "\n",
    "        # if p_human < alpha: # human rejects the null\n",
    "        #     if np.random.random() < alpha: # falsely rejected the null, type I error\n",
    "        #         continue\n",
    "        #     omissions += 1\n",
    "        #     if np.random.random() < beta: # failed to detect effect for auto. type II error\n",
    "        #         continue\n",
    "        #     false_negatives += p_auto > alpha # fail to reject null for auto\n",
    "        if p_auto < alpha and np.random.random() > alpha:\n",
    "            discoveries += 1\n",
    "            if np.random.random() > beta and p_human > alpha:\n",
    "                false_positives += 1\n",
    "        if p_human < alpha and np.random.random() > alpha:\n",
    "            omissions += 1\n",
    "            if np.random.random() > beta and p_auto > alpha:\n",
    "                false_negatives += 1\n",
    "    false_pos_rate = np.nan if discoveries == 0 else false_positives / discoveries\n",
    "    false_neg_rate = np.nan if omissions == 0 else false_negatives / omissions\n",
    "    return false_pos_rate, false_positives, discoveries, false_neg_rate, false_negatives, omissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1000/1000 [30:36<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.05\n",
    "BETA = 0.1\n",
    "N_ITERS = 1000\n",
    "NUM_TOPICS = 50\n",
    "MODELS = ['mallet', 'dvae', 'etm']\n",
    "\n",
    "np.random.seed(42)\n",
    "rows = []\n",
    "for iteration in tqdm(range(N_ITERS), total=N_ITERS):\n",
    "    for task in [\"ratings\", \"intrusions\"]:\n",
    "        for dataset in [\"wikitext\", \"nytimes\", \"all\"]:\n",
    "            data_df = task_data.copy().loc[task_data.task == task]\n",
    "            if dataset != \"all\":\n",
    "                data_df = data_df.loc[data_df.dataset == dataset]\n",
    "            if FILTER_ON_FAMILIARITY:\n",
    "                data_df = data_df.loc[data_df.confidences_raw == 1]\n",
    "\n",
    "            # run regressions for each metric\n",
    "            df = data_df.fillna(0)\n",
    "            # sample at the topic level\n",
    "            df = select_random_topics(df, NUM_TOPICS)\n",
    "\n",
    "            for i, metric in enumerate(auto_metric_names):\n",
    "                if dataset in metric:\n",
    "                    continue # don't re-do the internal for `wikitext_full`, `nytimes_full`\n",
    "\n",
    "                fp_rate, fp, pos, fn_rate, fn, neg = false_discovery_rate_sim(\n",
    "                    task,\n",
    "                    df,\n",
    "                    metric,\n",
    "                    alpha=ALPHA,\n",
    "                    beta=BETA,\n",
    "                    models=MODELS,\n",
    "                )\n",
    "                metric_base = re.search(\"c_npmi_10|c_v|c_uci|u_mass\", metric).group(0)\n",
    "                row = {\n",
    "                    \"task\": task,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"metric\": metric_base,\n",
    "                    \"reference\": metric.replace(f\"{metric_base}_\", \"\"),\n",
    "                    \"fp_rate\": fp_rate,\n",
    "                    \"false_positives\": fp,\n",
    "                    \"discoveries\": pos,\n",
    "                    \"fn_rate\": fn_rate,\n",
    "                    \"false_negatives\": fn,\n",
    "                    \"omissions\": neg,\n",
    "                }\n",
    "                rows.append(row)\n",
    "simulations = pd.DataFrame(rows)\n",
    "simulations.to_csv(f\"simple_simulation_results{familiar}.csv\", index=False)"
   ]
  },
  {
   "source": [
    "## Create latex table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = pd.read_csv(f\"simple_simulation_results{familiar}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations.loc[(simulations.dataset==\"wikitext\") & (simulations.reference == \"full\"), \"reference\"] = \"wikitext_full\"\n",
    "simulations.loc[(simulations.dataset==\"nytimes\") & (simulations.reference == \"full\"), \"reference\"] = \"nytimes_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_TO_KEEP = [\"c_npmi_10\", \"c_v\"]\n",
    "\n",
    "simulations_grouped = (\n",
    "    simulations.groupby([\"task\", \"dataset\", \"metric\", \"reference\"])[[\"false_positives\", \"discoveries\", 'false_negatives', 'omissions']]\n",
    "               .agg([\"sum\"])\n",
    "               .reset_index()\n",
    ")\n",
    "simulations_grouped[\"fp_rate\"] = simulations_grouped[\"false_positives\"] / simulations_grouped[\"discoveries\"]\n",
    "simulations_grouped[\"fn_rate\"] = simulations_grouped[\"false_negatives\"] / simulations_grouped[\"omissions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations_pivot = (\n",
    "    simulations_grouped.loc[simulations_grouped.metric.isin(METRICS_TO_KEEP)]\n",
    "               .sort_values([\"task\", \"dataset\", \"metric\", \"reference\"])\n",
    "               .pivot(index=[\"task\", \"dataset\"], columns=[\"metric\", \"reference\"], values=[\"fp_rate\", \"fn_rate\"])\n",
    ")\n",
    "simulations_pivot = simulations_pivot[[\n",
    "    (v, m, r)\n",
    "    for v in [\"fp_rate\", \"fn_rate\"]\n",
    "    for m in METRICS_TO_KEEP\n",
    "    for r in [\"nytimes_full\", \"wikitext_full\", \"train\"] #NB: test, val excluded\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the values\n",
    "for idx, row in simulations_pivot.iterrows():\n",
    "    newrow = []\n",
    "    prec = 1 - row[\"fp_rate\"]\n",
    "    rec = 1 - row[\"fn_rate\"]\n",
    "    f1s = 2 * ((prec * rec) / (prec + rec)) # calculate f1s\n",
    "\n",
    "    # format each value in the row\n",
    "    for i, (fpr, fnr, f1) in enumerate(zip(row[\"fp_rate\"], row[\"fn_rate\"], f1s)):\n",
    "        if np.isnan(fpr):\n",
    "            val = \"-\"\n",
    "        else:\n",
    "            val = f\"{fpr*100:0.0f}\"\n",
    "            # bold the max value\n",
    "            if f1 == np.max(f1s):\n",
    "                val = r\"\\textbf{\" + val + \"}\"\n",
    "            # add a leading zero if necessary\n",
    "            if len(val) < 2:\n",
    "                val = r\"\\phantom{0}\" + val\n",
    "            # include the false-negative rate\n",
    "            fnr_str = f\"{fnr*100:0.0f}\"\n",
    "            if len(fnr_str) < 2:\n",
    "                fnr_str = r\"\\phantom{0}\" + fnr_str\n",
    "            # put it all together\n",
    "            val += r\" / \\textcolor{gray}{\" + fnr_str + \"}\"\n",
    "        newrow.append(val)\n",
    "    simulations_pivot.at[idx, \"fp_rate\"] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\begin{tabular}{ll|rrr|rrr}\n\\toprule\n        &  & \\multicolumn{3}{c}{\\abr{npmi} (10-token window)} & \\multicolumn{3}{c}{$C_v$ (110-token window)} \\\\\n        & Ref. Corpus $\\rightarrow$ &                                   \\abr{nyt} &                                  \\abr{wiki} &                                          Train &                                   \\abr{nyt} &                                  \\abr{wiki} &                                          Train \\\\\n & Train Corpus $\\downarrow$ &                                                &                                                &                                                &                                                &                                                &                                                \\\\\n\\midrule\nIntrusion & \\abr{nyt} &  \\phantom{0}5 / \\textcolor{gray}{\\phantom{0}5} &            \\phantom{0}3 / \\textcolor{gray}{11} &  \\phantom{0}6 / \\textcolor{gray}{\\phantom{0}5} &  \\phantom{0}7 / \\textcolor{gray}{\\phantom{0}1} &  \\phantom{0}4 / \\textcolor{gray}{\\phantom{0}4} &    \\textbf{6} / \\textcolor{gray}{\\phantom{0}2} \\\\\n        & \\abr{wiki} &            14 / \\textcolor{gray}{\\phantom{0}9} &                      13 / \\textcolor{gray}{10} &                      13 / \\textcolor{gray}{12} &            14 / \\textcolor{gray}{\\phantom{0}9} &                      13 / \\textcolor{gray}{10} &   \\textbf{14} / \\textcolor{gray}{\\phantom{0}9} \\\\\n        & Concatenated &  \\phantom{0}8 / \\textcolor{gray}{\\phantom{0}3} &  \\phantom{0}5 / \\textcolor{gray}{\\phantom{0}5} &  \\phantom{0}8 / \\textcolor{gray}{\\phantom{0}3} &  \\phantom{0}8 / \\textcolor{gray}{\\phantom{0}2} &    \\textbf{7} / \\textcolor{gray}{\\phantom{0}2} &  \\phantom{0}8 / \\textcolor{gray}{\\phantom{0}2} \\\\\nRating & \\abr{nyt} &            16 / \\textcolor{gray}{\\phantom{0}9} &                      19 / \\textcolor{gray}{19} &                      16 / \\textcolor{gray}{10} &            16 / \\textcolor{gray}{\\phantom{0}5} &                      17 / \\textcolor{gray}{10} &   \\textbf{16} / \\textcolor{gray}{\\phantom{0}5} \\\\\n        & \\abr{wiki} &             \\textbf{21} / \\textcolor{gray}{17} &                      21 / \\textcolor{gray}{19} &                      20 / \\textcolor{gray}{20} &                      22 / \\textcolor{gray}{19} &                      22 / \\textcolor{gray}{20} &                      21 / \\textcolor{gray}{18} \\\\\n        & Concatenated &            15 / \\textcolor{gray}{\\phantom{0}4} &                      15 / \\textcolor{gray}{10} &            15 / \\textcolor{gray}{\\phantom{0}4} &   \\textbf{14} / \\textcolor{gray}{\\phantom{0}3} &            14 / \\textcolor{gray}{\\phantom{0}5} &            14 / \\textcolor{gray}{\\phantom{0}2} \\\\\n\\bottomrule\n\\end{tabular}\n\n"
     ]
    }
   ],
   "source": [
    "# make latex\n",
    "simulations_pivot_values = simulations_pivot[\"fp_rate\"].loc[[\n",
    "    ('intrusions',  'nytimes'),\n",
    "    ('intrusions', 'wikitext'),\n",
    "    ('intrusions',      'all'),\n",
    "    (   'ratings',  'nytimes'),\n",
    "    (   'ratings', 'wikitext'),\n",
    "    (   'ratings',      'all'),\n",
    "]]\n",
    "latex = simulations_pivot_values.to_latex(escape=False, multicolumn_format='c', column_format=\"ll|rrr|rrr\")\n",
    "\n",
    "for to_replace, val in LATEX_CLEANUP.items():\n",
    "    latex = latex.replace(to_replace, val)\n",
    "print(latex)\n"
   ]
  },
  {
   "source": [
    "## Regression "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AVERAGE_ANNOTATIONS = False\n",
    "DROP_NA = False\n",
    "USE_OLS = False\n",
    "ALPHA = 0.05\n",
    "\n",
    "np.random.seed(42)\n",
    "rows = []\n",
    "for task in [\"ratings\", \"intrusions\"]:\n",
    "    for dataset in [\"wikitext\", \"nytimes\", \"all\"]:\n",
    "        data_df = task_data.loc[task_data.task == task]\n",
    "        if dataset != \"all\":\n",
    "            data_df = data_df.loc[data_df.dataset == dataset]\n",
    "        if FILTER_ON_FAMILIARITY:\n",
    "            data_df = data_df.loc[data_df.confidences_raw == 1]\n",
    "\n",
    "        # run regressions for each metric\n",
    "        for i, metric in enumerate(auto_metric_names):\n",
    "            if dataset in metric:\n",
    "                continue # don't re-do the internal for `wikitext_full`, `nytimes_full`\n",
    "            if DROP_NA:\n",
    "                df = data_df.dropna(subset=[metric])\n",
    "            else:\n",
    "                df = data_df.fillna(0)\n",
    "            if AVERAGE_ANNOTATIONS:\n",
    "                df = df.groupby([\"model\", \"topic_idx\"]).mean().reset_index()\n",
    "                mod = sm.OLS(df[\"scores_raw\"], df[[\"const\", metric]])\n",
    "            if USE_OLS:\n",
    "                scores = df[\"scores_raw\"] if task == \"intrusions\" else (df[\"scores_raw\"] - 1) / 2\n",
    "                mod = sm.OLS(scores, df[[\"const\", metric]])\n",
    "            elif task == \"intrusions\":\n",
    "                mod = sm.Logit(df[\"scores_raw\"], df[[\"const\", metric]])\n",
    "            elif task == \"ratings\":\n",
    "                mod = OrderedModel(df[\"scores_raw\"], df[[metric]], distr=\"probit\")\n",
    "            res = mod.fit(disp=0)\n",
    "            metric_base = re.search(\"c_npmi_10|c_v|c_uci|u_mass\", metric).group(0)\n",
    "            ci_lb, ci_ub = res.conf_int(alpha=ALPHA).loc[metric] \n",
    "            row = {\n",
    "                \"task\": task,\n",
    "                \"dataset\": dataset,\n",
    "                \"metric\": metric_base,\n",
    "                \"reference\": metric.replace(f\"{metric_base}_\", \"\"),\n",
    "                \"coef\": res.params[metric],\n",
    "                \"se\": res.bse[metric],\n",
    "                \"p\": res.pvalues[metric],\n",
    "                \"bic\": res.bic,\n",
    "                \"ci_0.025\": ci_lb,\n",
    "                \"ci_0.975\": ci_ub,\n",
    "            }\n",
    "            rows.append(row)\n",
    "regressions = pd.DataFrame(rows)\n",
    "regressions.to_csv(f\"regression_results{familiar}.csv\", index=False)"
   ]
  },
  {
   "source": [
    "## Make the latex table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "regressions = pd.read_csv(f\"regression_results{familiar}.csv\", index=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressions.loc[(regressions.dataset==\"wikitext\") & (regressions.reference == \"full\"), \"reference\"] = \"wikitext_full\"\n",
    "regressions.loc[(regressions.dataset==\"nytimes\") & (regressions.reference == \"full\"), \"reference\"] = \"nytimes_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_TO_KEEP = [\"c_npmi_10\", \"c_v\"]\n",
    "\n",
    "regressions_pivot = (\n",
    "    regressions.loc[regressions.metric.isin(METRICS_TO_KEEP)]\n",
    "               .replace({\"\"})\n",
    "               .sort_values([\"task\", \"dataset\", \"metric\", \"reference\"])\n",
    "               .pivot(index=[\"task\", \"dataset\"], columns=[\"metric\", \"reference\"], values=[\"coef\", \"ci_0.025\", \"ci_0.975\"])\n",
    ")\n",
    "# order the columns\n",
    "regressions_pivot = regressions_pivot[[\n",
    "    (v, m, r)\n",
    "    for v in [\"coef\", \"ci_0.025\", \"ci_0.975\"]\n",
    "    for m in METRICS_TO_KEEP\n",
    "    for r in [\"nytimes_full\", \"wikitext_full\", \"train\", \"val\"] #NB: test, full excluded\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bold format the significant values\n",
    "for idx, row in regressions_pivot.iterrows():\n",
    "    max_coefs = find_max_values(row['coef'], row[\"ci_0.025\"], row[\"ci_0.975\"])\n",
    "    newrow = []\n",
    "    for i, x in enumerate(row[\"coef\"]):\n",
    "        if np.isnan(x):\n",
    "            val = \"-\"\n",
    "        elif not np.isnan(x) and max_coefs[i]:\n",
    "            val = r\"\\uline{\" + f\"{x:0.2f}\" + \"}\"\n",
    "            if x == np.max(row['coef']):\n",
    "                val = r\"\\textbf{\" + val + \"}\"\n",
    "        else:\n",
    "            val = f\"{x:0.2f}\"\n",
    "        newrow.append(val)\n",
    "    regressions_pivot.at[idx, \"coef\"] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\begin{tabular}{ll|rrrr|rrrr}\n\\toprule\n        &  & \\multicolumn{4}{c}{\\abr{npmi} (10-token window)} & \\multicolumn{4}{c}{$C_v$ (110-token window)} \\\\\n        & Ref. Corpus $\\rightarrow$ &  \\abr{nyt} &          \\abr{wiki} &         Train &   Val & \\abr{nyt} & \\abr{wiki} & Train &   Val \\\\\n & Train Corpus $\\downarrow$ &               &                        &               &       &              &               &       &       \\\\\n\\midrule\nIntrusion & \\abr{nyt} &          3.71 &  \\textbf{\\uline{7.14}} &          3.04 &  2.54 &         3.34 &          4.54 &  3.23 &  2.94 \\\\\n        & \\abr{wiki} &  \\uline{5.87} &  \\textbf{\\uline{6.46}} &  \\uline{6.19} &  0.85 &         3.23 &          3.59 &  3.39 &  0.42 \\\\\n        & Both &          4.24 &  \\textbf{\\uline{6.81}} &          4.17 &  0.94 &         3.18 &          4.06 &  3.30 &  0.91 \\\\\nRating & \\abr{nyt} &          4.40 &  \\textbf{\\uline{5.87}} &          3.85 &  3.93 &         3.97 &          4.44 &  4.03 &  3.89 \\\\\n        & \\abr{wiki} &  \\uline{4.84} &  \\textbf{\\uline{5.95}} &  \\uline{5.65} &  1.33 &         2.96 &          3.73 &  3.69 &  0.62 \\\\\n        & Both &          4.49 &  \\textbf{\\uline{5.80}} &          4.56 &  1.78 &         3.45 &          3.91 &  3.81 &  1.32 \\\\\n\\bottomrule\n\\end{tabular}\n\n"
     ]
    }
   ],
   "source": [
    "# make latex\n",
    "regressions_pivot_coefs = regressions_pivot[\"coef\"].loc[[\n",
    "    ('intrusions',  'nytimes'),\n",
    "    ('intrusions', 'wikitext'),\n",
    "    ('intrusions',      'all'),\n",
    "    (   'ratings',  'nytimes'),\n",
    "    (   'ratings', 'wikitext'),\n",
    "    (   'ratings',      'all'),\n",
    "]]\n",
    "latex = regressions_pivot_coefs.to_latex(escape=False, multicolumn_format='c', column_format=\"ll|rrrr|rrrr\")\n",
    "\n",
    "for to_replace, val in LATEX_CLEANUP.items():\n",
    "    latex = latex.replace(to_replace, val)\n",
    "print(latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}