{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "statsmodels-dev",
   "display_name": "statsmodels-dev",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels.miscmodels.ordinal_model'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-99464b49b874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiscmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordinal_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproportion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproportions_ztest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels.miscmodels.ordinal_model'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, mannwhitneyu, ttest_ind"
   ]
  },
  {
   "source": [
    "# Setup\n",
    "## Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/human/all_data\"\n",
    "DATA_FILE = \"all_data.csv\"\n",
    "\n",
    "FILTER_ON_FAMILIARITY = False\n",
    "familiar = \"_familiar\" if FILTER_ON_FAMILIARITY else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_metric_names = [\n",
    " 'c_npmi_10_full',\n",
    " 'c_npmi_10_nytimes_full',\n",
    " #'c_npmi_10_test',\n",
    " 'c_npmi_10_train',\n",
    " 'c_npmi_10_val',\n",
    " 'c_npmi_10_wikitext_full',\n",
    " #'c_uci_full',\n",
    " 'c_v_full',\n",
    " 'c_v_nytimes_full',\n",
    " #'c_v_test',\n",
    " 'c_v_train',\n",
    " 'c_v_val',\n",
    " 'c_v_wikitext_full',\n",
    " #'u_mass_full'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data = pd.read_csv(Path(DATA_DIR, DATA_FILE))\n",
    "\n",
    "# complete the out-of-sample columns\n",
    "for dataset in [\"wikitext\", \"nytimes\"]:\n",
    "    for metric in [\"c_npmi_10\", \"c_v\"]:\n",
    "        task_data[f\"{metric}_{dataset}_full\"] = task_data[f\"{metric}_{dataset}_full\"].combine_first(task_data[f\"{metric}_full\"])\n",
    "\n",
    "# add the constant for lin. reg\n",
    "task_data = sm.add_constant(task_data)\n",
    "\n",
    "# convert infinite values to nans\n",
    "task_data = task_data.replace(np.inf, np.nan)"
   ]
  },
  {
   "source": [
    "## Helper functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_values(values, lbs, ubs):\n",
    "    \"\"\"\n",
    "    Given arrays of `values` and associated lower (`lbs`) and upper bounds (`ubs`),\n",
    "    for confidence intervals, return binary array of indices of `values` that have \n",
    "    CIs that overlap with the CI of the maximum value.\n",
    "    \"\"\"\n",
    "    n = len(values)\n",
    "    values = np.nan_to_num(values, nan=0)\n",
    "    lbs = np.nan_to_num(lbs, nan=0)\n",
    "    ubs = np.nan_to_num(ubs, nan=0)\n",
    "    sorted_value_idx = np.argsort(values)[::-1]\n",
    "    max_lbs = lbs[sorted_value_idx[0]]\n",
    "    for i, idx in enumerate(sorted_value_idx[1:], start=1):\n",
    "        # is the upper bound of this value contained in the\n",
    "        # lower bound of the largest coefficient?\n",
    "        if ubs[idx] < max_lbs:\n",
    "            return np.array([j in sorted_value_idx[:i] for j in range(n)])\n",
    "    return np.full(n, True)\n",
    "\n",
    "def find_min_values(values, lbs, ubs):\n",
    "    \"\"\"\n",
    "    Analogous to `find_max_values`\n",
    "    \"\"\"\n",
    "    return find_max_values(-values, -ubs, -lbs, contiguous=contiguous)\n",
    "\n",
    "def ci_lb(x):\n",
    "    return np.quantile(x, 0.025)\n",
    "\n",
    "def ci_ub(x):\n",
    "    return np.quantile(x, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEX_CLEANUP = {\n",
    "    \"c_v\": r\"$C_v$ (110-token window)\",\n",
    "    \"test\": \"Test\",\n",
    "    \"c_npmi_10\": r\"\\abr{npmi} (10-token window)\",\n",
    "    \"nytimes_full\": r\"\\abr{nyt}\",\n",
    "    \"wikitext_full\": r\"\\abr{wiki}\",\n",
    "    \"wikitext\": r\"\\abr{wiki}\",\n",
    "    \"nytimes\": r\"\\abr{nyt}\",\n",
    "    \"all\": \"Concatenated\",\n",
    "    \"full\": \"Full\",\n",
    "    \"train\": \"Train\",\n",
    "    \"val\": \"Val\",\n",
    "    \"test\": \"Test\",\n",
    "    \"ratings\": \"Rating\",\n",
    "    \"intrusions\": \"Intrusion\",\n",
    "    \"metric\": \"\",\n",
    "    \"dataset\": r\"Train Corpus $\\downarrow$\",\n",
    "    \"reference\": r\"Ref. Corpus $\\rightarrow$\",\n",
    "    \"task\": \"\",\n",
    "}"
   ]
  },
  {
   "source": [
    "# Bootstrapped correlations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [01:17<00:00,  7.79s/it]\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.05\n",
    "N_ITERS = 500\n",
    "NUM_ANNOTATORS = {\"intrusions\": 26, \"ratings\": 15}\n",
    "\n",
    "np.random.seed(42)\n",
    "rows = []\n",
    "for iteration in tqdm(range(N_ITERS), total=N_ITERS):\n",
    "    for task in [\"ratings\", \"intrusions\"]:\n",
    "        for dataset in [\"wikitext\", \"nytimes\", \"all\"]:\n",
    "            data_df = task_data.loc[task_data.task == task]\n",
    "            if dataset != \"all\":\n",
    "                data_df = data_df.loc[data_df.dataset == dataset]\n",
    "            if FILTER_ON_FAMILIARITY:\n",
    "                data_df = data_df.loc[data_df.confidences_raw == 1]\n",
    "\n",
    "            # run correlations for each metric\n",
    "            for i, metric in enumerate(auto_metric_names):\n",
    "                if dataset in metric:\n",
    "                    continue # don't re-do the internal\n",
    "                if DROP_NA:\n",
    "                    df = data_df.dropna(subset=[metric])\n",
    "                else:\n",
    "                    df = data_df.fillna(0)\n",
    "                # sample \n",
    "                df = (\n",
    "                    df.groupby([\"task\", \"dataset\", \"model\", \"topic_idx\"])\n",
    "                      .sample(NUM_ANNOTATORS[task], replace=True)\n",
    "                      .groupby([\"task\", \"dataset\", \"model\", \"topic_idx\"])[[metric, \"scores_raw\"]]\n",
    "                      .mean()\n",
    "                )\n",
    "                spear_rho, spear_p = spearmanr(df[metric].values, df[\"scores_raw\"].values)\n",
    "                pear_rho, pear_p = pearsonr(df[metric].values, df[\"scores_raw\"].values)\n",
    "                metric_base = re.search(\"c_npmi_10|c_v|c_uci|u_mass\", metric).group(0)\n",
    "                row = {\n",
    "                    \"task\": task,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"metric\": metric_base,\n",
    "                    \"reference\": metric.replace(f\"{metric_base}_\", \"\"),\n",
    "                    \"spear_rho\": spear_rho,\n",
    "                    \"pear_rho\": pear_rho,\n",
    "                    \"spear_p\": spear_p,\n",
    "                    \"pear_p\": pear_p,\n",
    "                }\n",
    "                rows.append(row)\n",
    "correlations = pd.DataFrame(rows)\n",
    "correlations.to_csv(f\"correlation_results{familiar}.csv\", index=False)"
   ]
  },
  {
   "source": [
    "## Create table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = pd.read_csv(f\"correlation_results{familiar}.csv\")\n",
    "correlations.loc[(correlations.dataset==\"wikitext\") & (correlations.reference == \"full\"), \"reference\"] = \"wikitext_full\"\n",
    "correlations.loc[(correlations.dataset==\"nytimes\") & (correlations.reference == \"full\"), \"reference\"] = \"nytimes_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_COL = \"spear_rho\"\n",
    "METRICS_TO_KEEP = [\"c_npmi_10\", \"c_v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_grouped = (\n",
    "    correlations.groupby([\"task\", \"dataset\", \"metric\", \"reference\"])[VALUE_COL]\n",
    "               .agg([\"mean\", \"std\", ci_lb, ci_ub])\n",
    "               .reset_index()\n",
    "               .rename(columns={\"mean\": VALUE_COL, \"ci_lb\": \"ci_0.025\", \"ci_ub\": \"ci_0.975\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_pivot = (\n",
    "    correlations_grouped.loc[correlations_grouped.metric.isin(METRICS_TO_KEEP)]\n",
    "               .sort_values([\"task\", \"dataset\", \"metric\", \"reference\"])\n",
    "               .pivot(index=[\"task\", \"dataset\"], columns=[\"metric\", \"reference\"], values=[VALUE_COL, \"ci_0.025\",  \"ci_0.975\"])\n",
    ")\n",
    "correlations_pivot = correlations_pivot[[\n",
    "    (v, m, r)\n",
    "    for v in [VALUE_COL, \"ci_0.025\",  \"ci_0.975\"]\n",
    "    for m in METRICS_TO_KEEP\n",
    "    for r in [\"nytimes_full\", \"wikitext_full\", \"train\", \"val\"]\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in correlations_pivot.iterrows():\n",
    "    # determine overlapping CIs\n",
    "    max_values = find_max_values(row[VALUE_COL], row[\"ci_0.025\"], row[\"ci_0.975\"])\n",
    "    newrow = []\n",
    "    # format each item in the row,\n",
    "    # bold for max value, underline for overlapping\n",
    "    for i, x in enumerate(row[VALUE_COL]):\n",
    "        if np.isnan(x):\n",
    "            val = \"-\"\n",
    "        elif not np.isnan(x) and max_values[i]:\n",
    "            val = r\"\\uline{\" + f\"{x:0.2f}\" + \"}\"\n",
    "            if x == np.max(row[VALUE_COL]):\n",
    "                val = r\"\\textbf{\" + val + \"}\"\n",
    "        else:\n",
    "            val = f\"{x:0.2f}\"\n",
    "        newrow.append(val)\n",
    "    correlations_pivot.at[idx, VALUE_COL] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\begin{tabular}{ll|rrrr|rrrr}\n\\toprule\n        &  & \\multicolumn{4}{c}{\\abr{npmi} (10-token window)} & \\multicolumn{4}{c}{$C_v$ (110-token window)} \\\\\n        & Ref. Corpus $\\rightarrow$ &  \\abr{nyt} &          \\abr{wiki} &         Train &   Val &  \\abr{nyt} &          \\abr{wiki} &         Train &   Val \\\\\n & Train Corpus $\\downarrow$ &               &                        &               &       &               &                        &               &       \\\\\n\\midrule\nIntrusion & \\abr{nyt} &          0.34 &           \\uline{0.51} &          0.32 &  0.25 &          0.44 &  \\textbf{\\uline{0.56}} &          0.42 &  0.38 \\\\\n        & \\abr{wiki} &  \\uline{0.39} &           \\uline{0.39} &  \\uline{0.40} &  0.14 &  \\uline{0.38} &  \\textbf{\\uline{0.40}} &  \\uline{0.39} &  0.13 \\\\\n        & Both &          0.36 &           \\uline{0.45} &          0.36 &  0.18 &          0.41 &  \\textbf{\\uline{0.48}} &          0.41 &  0.26 \\\\\nRating & \\abr{nyt} &          0.45 &  \\textbf{\\uline{0.59}} &          0.44 &  0.43 &          0.51 &           \\uline{0.58} &  \\uline{0.53} &  0.52 \\\\\n        & \\abr{wiki} &  \\uline{0.45} &  \\textbf{\\uline{0.51}} &  \\uline{0.51} &  0.21 &  \\uline{0.44} &           \\uline{0.51} &  \\uline{0.51} &  0.23 \\\\\n        & Both &          0.47 &  \\textbf{\\uline{0.54}} &          0.47 &  0.35 &  \\uline{0.49} &           \\uline{0.53} &  \\uline{0.51} &  0.42 \\\\\n\\bottomrule\n\\end{tabular}\n\n"
     ]
    }
   ],
   "source": [
    "# make latex\n",
    "correlations_pivot_values = correlations_pivot[VALUE_COL].loc[[\n",
    "    ('intrusions',  'nytimes'),\n",
    "    ('intrusions', 'wikitext'),\n",
    "    ('intrusions',      'all'),\n",
    "    (   'ratings',  'nytimes'),\n",
    "    (   'ratings', 'wikitext'),\n",
    "    (   'ratings',      'all'),\n",
    "]]\n",
    "latex = correlations_pivot_values.to_latex(escape=False, multicolumn_format='c', column_format=\"ll|rrrr|rrrr\")\n",
    "for to_replace, val in LATEX_CLEANUP.items():\n",
    "    latex = latex.replace(to_replace, val)\n",
    "print(latex)\n"
   ]
  },
  {
   "source": [
    "# Simple Simulation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_topics(df, num_topics=50):\n",
    "    \"\"\"\n",
    "    Sample `num_topics` random topics, with replacement, for each \n",
    "    (task, dataset, model) tuple\n",
    "    \"\"\"\n",
    "    topic_sample = (\n",
    "        df.groupby([\"task\", \"dataset\", \"model\", \"topic_idx\"])\n",
    "                    .size()\n",
    "                    .reset_index()\n",
    "                    .groupby([\"task\", \"dataset\", \"model\"])\n",
    "                    .sample(num_topics, replace=True)\n",
    "                    .drop(columns=0)\n",
    "    )\n",
    "    topic_sample[\"rand_topic_idx\"] = topic_sample.groupby(['task', 'dataset', 'model']).cumcount()\n",
    "    df = df.merge(topic_sample, how='inner')\n",
    "    return df\n",
    "\n",
    "def intrusion_test(scores_a, scores_b, alternative=\"larger\"):\n",
    "    \"\"\"\n",
    "    Proportions test of difference in intrusion scores\n",
    "    \"\"\"\n",
    "    return proportions_ztest(\n",
    "        [scores_a.sum(), scores_b.sum()],\n",
    "        [len(scores_a), len(scores_b)],\n",
    "        alternative=alternative\n",
    "    )\n",
    "\n",
    "def ratings_test(scores_a, scores_b, alternative=\"greater\"):\n",
    "    \"\"\"\n",
    "    Mann-Whitney U-test of difference in ratings cores\n",
    "    \"\"\"\n",
    "    return mannwhitneyu(scores_a, scores_b, alternative=alternative)\n",
    "\n",
    "def auto_test(scores_a, scores_b, alternative=\"greater\"):\n",
    "    \"\"\"\n",
    "    t-test of difference in automated scores\n",
    "    \"\"\"\n",
    "    return ttest_ind(scores_a, scores_b, equal_var=False, alternative=alternative)\n",
    "\n",
    "def false_discovery_rate_sim(\n",
    "    task,\n",
    "    test_df,\n",
    "    metric,\n",
    "    alpha=0.05,\n",
    "    beta=0.1,\n",
    "    models=[\"mallet\", \"dvae\", \"etm\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a sample of data, determine the number of times\n",
    "    that auto metrics imply a significant difference between models\n",
    "    when the human scores do not.\n",
    "    \"\"\"\n",
    "    false_positives, false_negatives = 0, 0\n",
    "    discoveries, omissions = 0, 0\n",
    "    if task == \"intrusions\":\n",
    "        stat_test = intrusion_test\n",
    "    elif task == \"ratings\":\n",
    "        stat_test = ratings_test\n",
    "    # model scores are the same across all the human ratings\n",
    "    model_scores = test_df.groupby(['task', 'dataset', 'model', 'rand_topic_idx']).head(1)\n",
    "\n",
    "    # Calculate the false discovery rate\n",
    "    for model_a, model_b in itertools.permutations(models, 2):\n",
    "        model_a_idxr = test_df.model==model_a\n",
    "        model_b_idxr = test_df.model==model_b\n",
    "        # run the tests for both human and auto metrics\n",
    "        stat_auto, p_auto = auto_test(\n",
    "            model_scores.loc[model_scores.model==model_a][metric],\n",
    "            model_scores.loc[model_scores.model==model_b][metric],\n",
    "        )\n",
    "        stat_human, p_human = stat_test(\n",
    "            test_df.loc[test_df.model==model_a][\"scores_raw\"],\n",
    "            test_df.loc[test_df.model==model_b][\"scores_raw\"],\n",
    "        )\n",
    "\n",
    "        # Count the disagreements while controlling for baseline\n",
    "        # probabilities of type I and II errors\n",
    "        # if p_auto < alpha: # rejects the null based on auto scores\n",
    "        #     if np.random.random() < alpha:\n",
    "        #         continue # falsely rejected the null, type I error\n",
    "        #     discoveries += 1\n",
    "        #     if np.random.random() < beta: # failed to detect effect for human. type II error\n",
    "        #         continue\n",
    "        #     false_positives += p_human > alpha # fail to reject the null for auto\n",
    "\n",
    "        # if p_human < alpha: # human rejects the null\n",
    "        #     if np.random.random() < alpha: # falsely rejected the null, type I error\n",
    "        #         continue\n",
    "        #     omissions += 1\n",
    "        #     if np.random.random() < beta: # failed to detect effect for auto. type II error\n",
    "        #         continue\n",
    "        #     false_negatives += p_auto > alpha # fail to reject null for auto\n",
    "        if p_auto < alpha and np.random.random() > alpha:\n",
    "            discoveries += 1\n",
    "            if np.random.random() > beta and p_human > alpha:\n",
    "                false_positives += 1\n",
    "        if p_human < alpha and np.random.random() > alpha:\n",
    "            omissions += 1\n",
    "            if np.random.random() > beta and p_auto > alpha:\n",
    "                false_negatives += 1\n",
    "    false_pos_rate = np.nan if discoveries == 0 else false_positives / discoveries\n",
    "    false_neg_rate = np.nan if omissions == 0 else false_negatives / omissions\n",
    "    return false_pos_rate, false_positives, discoveries, false_neg_rate, false_negatives, omissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  2%|▏         | 21/1000 [00:41<31:54,  1.96s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-1c49e3575615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0;32mcontinue\u001b[0m \u001b[0;31m# don't re-do the internal for `wikitext_full`, `nytimes_full`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 fp_rate, fp, pos, fn_rate, fn, neg = false_discovery_rate_sim(\n\u001b[0m\u001b[1;32m     28\u001b[0m                     \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f3e2cff725e4>\u001b[0m in \u001b[0;36mfalse_discovery_rate_sim\u001b[0;34m(task, test_df, metric, alpha, beta, models)\u001b[0m\n\u001b[1;32m     71\u001b[0m         stat_human, p_human = stat_test(\n\u001b[1;32m     72\u001b[0m             \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmodel_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scores_raw\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmodel_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scores_raw\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/envs/topic-preprocessing/lib/python3.9/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/envs/topic-preprocessing/lib/python3.9/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/envs/topic-preprocessing/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   4973\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4975\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4977\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/envs/topic-preprocessing/lib/python3.9/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.conda/envs/topic-preprocessing/lib/python3.9/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ALPHA = 0.05\n",
    "BETA = 0.1\n",
    "N_ITERS = 1000\n",
    "NUM_TOPICS = 50\n",
    "MODELS = ['mallet', 'dvae', 'etm']\n",
    "\n",
    "np.random.seed(11235)\n",
    "rows = []\n",
    "for iteration in tqdm(range(N_ITERS), total=N_ITERS):\n",
    "    for task in [\"ratings\", \"intrusions\"]:\n",
    "        for dataset in [\"wikitext\", \"nytimes\", \"all\"]:\n",
    "            data_df = task_data.copy().loc[task_data.task == task]\n",
    "            if dataset != \"all\":\n",
    "                data_df = data_df.loc[data_df.dataset == dataset]\n",
    "            if FILTER_ON_FAMILIARITY:\n",
    "                data_df = data_df.loc[data_df.confidences_raw == 1]\n",
    "\n",
    "            # run regressions for each metric\n",
    "            df = data_df.fillna(0)\n",
    "            # sample at the topic level\n",
    "            df = select_random_topics(df, NUM_TOPICS)\n",
    "\n",
    "            for i, metric in enumerate(auto_metric_names):\n",
    "                if dataset in metric:\n",
    "                    continue # don't re-do the internal for `wikitext_full`, `nytimes_full`\n",
    "\n",
    "                fp_rate, fp, pos, fn_rate, fn, neg = false_discovery_rate_sim(\n",
    "                    task,\n",
    "                    df,\n",
    "                    metric,\n",
    "                    alpha=ALPHA,\n",
    "                    beta=BETA,\n",
    "                    models=MODELS,\n",
    "                )\n",
    "                metric_base = re.search(\"c_npmi_10|c_v|c_uci|u_mass\", metric).group(0)\n",
    "                row = {\n",
    "                    \"task\": task,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"metric\": metric_base,\n",
    "                    \"reference\": metric.replace(f\"{metric_base}_\", \"\"),\n",
    "                    \"fp_rate\": fp_rate,\n",
    "                    \"false_positives\": fp,\n",
    "                    \"discoveries\": pos,\n",
    "                    \"fn_rate\": fn_rate,\n",
    "                    \"false_negatives\": fn,\n",
    "                    \"omissions\": neg,\n",
    "                }\n",
    "                rows.append(row)\n",
    "simulations = pd.DataFrame(rows)\n",
    "simulations.to_csv(f\"simple_simulation_results_11235{familiar}.csv\", index=False)"
   ]
  },
  {
   "source": [
    "## Create latex table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = pd.read_csv(f\"simple_simulation_results{familiar}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             task   dataset     metric      reference   fp_rate  \\\n",
       "0         ratings  wikitext  c_npmi_10           full  1.000000   \n",
       "1         ratings  wikitext  c_npmi_10   nytimes_full  0.333333   \n",
       "2         ratings  wikitext  c_npmi_10          train  1.000000   \n",
       "3         ratings  wikitext  c_npmi_10            val  0.666667   \n",
       "4         ratings  wikitext        c_v           full  0.333333   \n",
       "...           ...       ...        ...            ...       ...   \n",
       "51995  intrusions       all        c_v           full  0.000000   \n",
       "51996  intrusions       all        c_v   nytimes_full  0.000000   \n",
       "51997  intrusions       all        c_v          train  0.000000   \n",
       "51998  intrusions       all        c_v            val  1.000000   \n",
       "51999  intrusions       all        c_v  wikitext_full  0.333333   \n",
       "\n",
       "       false_positives  discoveries   fn_rate  false_negatives  omissions  \n",
       "0                    2            2  1.000000                3          3  \n",
       "1                    1            3  0.333333                1          3  \n",
       "2                    2            2  0.000000                0          1  \n",
       "3                    2            3  0.500000                1          2  \n",
       "4                    1            3  0.333333                1          3  \n",
       "...                ...          ...       ...              ...        ...  \n",
       "51995                0            3  0.000000                0          3  \n",
       "51996                0            3  0.000000                0          3  \n",
       "51997                0            2  0.000000                0          2  \n",
       "51998                3            3  1.000000                3          3  \n",
       "51999                1            3  0.000000                0          1  \n",
       "\n",
       "[52000 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>task</th>\n      <th>dataset</th>\n      <th>metric</th>\n      <th>reference</th>\n      <th>fp_rate</th>\n      <th>false_positives</th>\n      <th>discoveries</th>\n      <th>fn_rate</th>\n      <th>false_negatives</th>\n      <th>omissions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>full</td>\n      <td>1.000000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1.000000</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>nytimes_full</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>train</td>\n      <td>1.000000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>val</td>\n      <td>0.666667</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_v</td>\n      <td>full</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>51995</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>full</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>51996</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>nytimes_full</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>51997</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>train</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>51998</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>val</td>\n      <td>1.000000</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1.000000</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>51999</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>wikitext_full</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>52000 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             task   dataset     metric      reference   fp_rate  \\\n",
       "0         ratings  wikitext  c_npmi_10           full  1.000000   \n",
       "1         ratings  wikitext  c_npmi_10   nytimes_full  0.333333   \n",
       "2         ratings  wikitext  c_npmi_10          train  1.000000   \n",
       "3         ratings  wikitext  c_npmi_10            val  0.666667   \n",
       "4         ratings  wikitext        c_v           full  0.333333   \n",
       "...           ...       ...        ...            ...       ...   \n",
       "51995  intrusions       all        c_v           full  0.333333   \n",
       "51996  intrusions       all        c_v   nytimes_full  1.000000   \n",
       "51997  intrusions       all        c_v          train  0.000000   \n",
       "51998  intrusions       all        c_v            val  0.666667   \n",
       "51999  intrusions       all        c_v  wikitext_full  1.000000   \n",
       "\n",
       "       false_positives  discoveries   fn_rate  false_negatives  omissions  \n",
       "0                    2            2  1.000000                3          3  \n",
       "1                    1            3  0.333333                1          3  \n",
       "2                    2            2  0.000000                0          1  \n",
       "3                    2            3  0.500000                1          2  \n",
       "4                    1            3  0.333333                1          3  \n",
       "...                ...          ...       ...              ...        ...  \n",
       "51995                1            3  0.000000                0          1  \n",
       "51996                2            2  0.666667                2          3  \n",
       "51997                0            3  0.000000                0          2  \n",
       "51998                2            3  0.500000                1          2  \n",
       "51999                2            2       NaN                0          0  \n",
       "\n",
       "[52000 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>task</th>\n      <th>dataset</th>\n      <th>metric</th>\n      <th>reference</th>\n      <th>fp_rate</th>\n      <th>false_positives</th>\n      <th>discoveries</th>\n      <th>fn_rate</th>\n      <th>false_negatives</th>\n      <th>omissions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>full</td>\n      <td>1.000000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1.000000</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>nytimes_full</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>train</td>\n      <td>1.000000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>val</td>\n      <td>0.666667</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_v</td>\n      <td>full</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>51995</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>full</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>51996</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>nytimes_full</td>\n      <td>1.000000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.666667</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>51997</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>train</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>51998</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>val</td>\n      <td>0.666667</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>51999</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>wikitext_full</td>\n      <td>1.000000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>52000 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "simulations_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             task   dataset     metric      reference   fp_rate  \\\n",
       "0         ratings  wikitext  c_npmi_10  wikitext_full  1.000000   \n",
       "1         ratings  wikitext  c_npmi_10   nytimes_full  0.333333   \n",
       "2         ratings  wikitext  c_npmi_10          train  1.000000   \n",
       "3         ratings  wikitext  c_npmi_10            val  0.666667   \n",
       "4         ratings  wikitext        c_v  wikitext_full  0.333333   \n",
       "...           ...       ...        ...            ...       ...   \n",
       "51995  intrusions       all        c_v           full  0.000000   \n",
       "51996  intrusions       all        c_v   nytimes_full  0.500000   \n",
       "51997  intrusions       all        c_v          train  0.000000   \n",
       "51998  intrusions       all        c_v            val  0.500000   \n",
       "51999  intrusions       all        c_v  wikitext_full  0.333333   \n",
       "\n",
       "       false_positives  discoveries   fn_rate  false_negatives  omissions  \n",
       "0                    2            2  1.000000                3          3  \n",
       "1                    1            3  0.333333                1          3  \n",
       "2                    2            2  0.000000                0          1  \n",
       "3                    2            3  0.500000                1          2  \n",
       "4                    1            3  0.333333                1          3  \n",
       "...                ...          ...       ...              ...        ...  \n",
       "51995                0            3  0.000000                0          3  \n",
       "51996                1            2  0.000000                0          2  \n",
       "51997                0            3  0.000000                0          3  \n",
       "51998                1            2  0.666667                2          3  \n",
       "51999                1            3  0.000000                0          2  \n",
       "\n",
       "[52000 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>task</th>\n      <th>dataset</th>\n      <th>metric</th>\n      <th>reference</th>\n      <th>fp_rate</th>\n      <th>false_positives</th>\n      <th>discoveries</th>\n      <th>fn_rate</th>\n      <th>false_negatives</th>\n      <th>omissions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>wikitext_full</td>\n      <td>1.000000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1.000000</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>nytimes_full</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>train</td>\n      <td>1.000000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_npmi_10</td>\n      <td>val</td>\n      <td>0.666667</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ratings</td>\n      <td>wikitext</td>\n      <td>c_v</td>\n      <td>wikitext_full</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>51995</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>full</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>51996</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>nytimes_full</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>51997</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>train</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>51998</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>val</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.666667</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>51999</th>\n      <td>intrusions</td>\n      <td>all</td>\n      <td>c_v</td>\n      <td>wikitext_full</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>52000 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations.loc[(simulations.dataset==\"wikitext\") & (simulations.reference == \"full\"), \"reference\"] = \"wikitext_full\"\n",
    "simulations.loc[(simulations.dataset==\"nytimes\") & (simulations.reference == \"full\"), \"reference\"] = \"nytimes_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_TO_KEEP = [\"c_npmi_10\", \"c_v\"]\n",
    "\n",
    "simulations_grouped = (\n",
    "    simulations.groupby([\"task\", \"dataset\", \"metric\", \"reference\"])[[\"false_positives\", \"discoveries\", 'false_negatives', 'omissions']]\n",
    "               .agg([\"sum\"])\n",
    "               .reset_index()\n",
    ")\n",
    "simulations_grouped[\"fp_rate\"] = simulations_grouped[\"false_positives\"] / simulations_grouped[\"discoveries\"]\n",
    "simulations_grouped[\"fn_rate\"] = simulations_grouped[\"false_negatives\"] / simulations_grouped[\"omissions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations_pivot = (\n",
    "    simulations_grouped.loc[simulations_grouped.metric.isin(METRICS_TO_KEEP)]\n",
    "               .sort_values([\"task\", \"dataset\", \"metric\", \"reference\"])\n",
    "               .pivot(index=[\"task\", \"dataset\"], columns=[\"metric\", \"reference\"], values=[\"fp_rate\", \"fn_rate\"])\n",
    ")\n",
    "simulations_pivot = simulations_pivot[[\n",
    "    (v, m, r)\n",
    "    for v in [\"fp_rate\", \"fn_rate\"]\n",
    "    for m in METRICS_TO_KEEP\n",
    "    for r in [\"nytimes_full\", \"wikitext_full\", \"train\"] #NB: test, val excluded\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the values\n",
    "for idx, row in simulations_pivot.iterrows():\n",
    "    newrow = []\n",
    "    prec = 1 - row[\"fp_rate\"]\n",
    "    rec = 1 - row[\"fn_rate\"]\n",
    "    f1s = 2 * ((prec * rec) / (prec + rec)) # calculate f1s\n",
    "\n",
    "    # format each value in the row\n",
    "    for i, (fpr, fnr, f1) in enumerate(zip(row[\"fp_rate\"], row[\"fn_rate\"], f1s)):\n",
    "        if np.isnan(fpr):\n",
    "            val = \"-\"\n",
    "        else:\n",
    "            val = f\"{fpr*100:0.0f}\"\n",
    "            # bold the max value\n",
    "            if f1 == np.max(f1s):\n",
    "                val = r\"\\textbf{\" + val + \"}\"\n",
    "            # add a leading zero if necessary\n",
    "            if len(val) < 2:\n",
    "                val = r\"\\phantom{0}\" + val\n",
    "            # include the false-negative rate\n",
    "            fnr_str = f\"{fnr*100:0.0f}\"\n",
    "            if len(fnr_str) < 2:\n",
    "                fnr_str = r\"\\phantom{0}\" + fnr_str\n",
    "            # put it all together\n",
    "            val += r\" / \\textcolor{gray}{\" + fnr_str + \"}\"\n",
    "        newrow.append(val)\n",
    "    simulations_pivot.at[idx, \"fp_rate\"] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make latex\n",
    "simulations_pivot_values = simulations_pivot[\"fp_rate\"].loc[[\n",
    "    ('intrusions',  'nytimes'),\n",
    "    ('intrusions', 'wikitext'),\n",
    "    ('intrusions',      'all'),\n",
    "    (   'ratings',  'nytimes'),\n",
    "    (   'ratings', 'wikitext'),\n",
    "    (   'ratings',      'all'),\n",
    "]]\n",
    "latex = simulations_pivot_values.to_latex(escape=False, multicolumn_format='c', column_format=\"ll|rrr|rrr\")\n",
    "\n",
    "for to_replace, val in LATEX_CLEANUP.items():\n",
    "    latex = latex.replace(to_replace, val)\n",
    "print(latex)\n"
   ]
  },
  {
   "source": [
    "## Regression "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AVERAGE_ANNOTATIONS = False\n",
    "DROP_NA = False\n",
    "USE_OLS = False\n",
    "ALPHA = 0.05\n",
    "\n",
    "np.random.seed(42)\n",
    "rows = []\n",
    "for task in [\"ratings\", \"intrusions\"]:\n",
    "    for dataset in [\"wikitext\", \"nytimes\", \"all\"]:\n",
    "        data_df = task_data.loc[task_data.task == task]\n",
    "        if dataset != \"all\":\n",
    "            data_df = data_df.loc[data_df.dataset == dataset]\n",
    "        if FILTER_ON_FAMILIARITY:\n",
    "            data_df = data_df.loc[data_df.confidences_raw == 1]\n",
    "\n",
    "        # run regressions for each metric\n",
    "        for i, metric in enumerate(auto_metric_names):\n",
    "            if dataset in metric:\n",
    "                continue # don't re-do the internal for `wikitext_full`, `nytimes_full`\n",
    "            if DROP_NA:\n",
    "                df = data_df.dropna(subset=[metric])\n",
    "            else:\n",
    "                df = data_df.fillna(0)\n",
    "            if AVERAGE_ANNOTATIONS:\n",
    "                df = df.groupby([\"model\", \"topic_idx\"]).mean().reset_index()\n",
    "                mod = sm.OLS(df[\"scores_raw\"], df[[\"const\", metric]])\n",
    "            if USE_OLS:\n",
    "                scores = df[\"scores_raw\"] if task == \"intrusions\" else (df[\"scores_raw\"] - 1) / 2\n",
    "                mod = sm.OLS(scores, df[[\"const\", metric]])\n",
    "            elif task == \"intrusions\":\n",
    "                mod = sm.Logit(df[\"scores_raw\"], df[[\"const\", metric]])\n",
    "            elif task == \"ratings\":\n",
    "                mod = OrderedModel(df[\"scores_raw\"], df[[metric]], distr=\"probit\")\n",
    "            res = mod.fit(disp=0)\n",
    "            metric_base = re.search(\"c_npmi_10|c_v|c_uci|u_mass\", metric).group(0)\n",
    "            ci_lb, ci_ub = res.conf_int(alpha=ALPHA).loc[metric] \n",
    "            row = {\n",
    "                \"task\": task,\n",
    "                \"dataset\": dataset,\n",
    "                \"metric\": metric_base,\n",
    "                \"reference\": metric.replace(f\"{metric_base}_\", \"\"),\n",
    "                \"coef\": res.params[metric],\n",
    "                \"se\": res.bse[metric],\n",
    "                \"p\": res.pvalues[metric],\n",
    "                \"bic\": res.bic,\n",
    "                \"ci_0.025\": ci_lb,\n",
    "                \"ci_0.975\": ci_ub,\n",
    "            }\n",
    "            rows.append(row)\n",
    "regressions = pd.DataFrame(rows)\n",
    "regressions.to_csv(f\"regression_results{familiar}.csv\", index=False)"
   ]
  },
  {
   "source": [
    "## Make the latex table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "regressions = pd.read_csv(f\"regression_results{familiar}.csv\", index=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressions.loc[(regressions.dataset==\"wikitext\") & (regressions.reference == \"full\"), \"reference\"] = \"wikitext_full\"\n",
    "regressions.loc[(regressions.dataset==\"nytimes\") & (regressions.reference == \"full\"), \"reference\"] = \"nytimes_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_TO_KEEP = [\"c_npmi_10\", \"c_v\"]\n",
    "\n",
    "regressions_pivot = (\n",
    "    regressions.loc[regressions.metric.isin(METRICS_TO_KEEP)]\n",
    "               .replace({\"\"})\n",
    "               .sort_values([\"task\", \"dataset\", \"metric\", \"reference\"])\n",
    "               .pivot(index=[\"task\", \"dataset\"], columns=[\"metric\", \"reference\"], values=[\"coef\", \"ci_0.025\", \"ci_0.975\"])\n",
    ")\n",
    "# order the columns\n",
    "regressions_pivot = regressions_pivot[[\n",
    "    (v, m, r)\n",
    "    for v in [\"coef\", \"ci_0.025\", \"ci_0.975\"]\n",
    "    for m in METRICS_TO_KEEP\n",
    "    for r in [\"nytimes_full\", \"wikitext_full\", \"train\", \"val\"] #NB: test, full excluded\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bold format the significant values\n",
    "for idx, row in regressions_pivot.iterrows():\n",
    "    max_coefs = find_max_values(row['coef'], row[\"ci_0.025\"], row[\"ci_0.975\"])\n",
    "    newrow = []\n",
    "    for i, x in enumerate(row[\"coef\"]):\n",
    "        if np.isnan(x):\n",
    "            val = \"-\"\n",
    "        elif not np.isnan(x) and max_coefs[i]:\n",
    "            val = r\"\\uline{\" + f\"{x:0.2f}\" + \"}\"\n",
    "            if x == np.max(row['coef']):\n",
    "                val = r\"\\textbf{\" + val + \"}\"\n",
    "        else:\n",
    "            val = f\"{x:0.2f}\"\n",
    "        newrow.append(val)\n",
    "    regressions_pivot.at[idx, \"coef\"] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\begin{tabular}{ll|rrrr|rrrr}\n\\toprule\n        &  & \\multicolumn{4}{c}{\\abr{npmi} (10-token window)} & \\multicolumn{4}{c}{$C_v$ (110-token window)} \\\\\n        & Ref. Corpus $\\rightarrow$ &  \\abr{nyt} &          \\abr{wiki} &         Train &   Val & \\abr{nyt} & \\abr{wiki} & Train &   Val \\\\\n & Train Corpus $\\downarrow$ &               &                        &               &       &              &               &       &       \\\\\n\\midrule\nIntrusion & \\abr{nyt} &          3.71 &  \\textbf{\\uline{7.14}} &          3.04 &  2.54 &         3.34 &          4.54 &  3.23 &  2.94 \\\\\n        & \\abr{wiki} &  \\uline{5.87} &  \\textbf{\\uline{6.46}} &  \\uline{6.19} &  0.85 &         3.23 &          3.59 &  3.39 &  0.42 \\\\\n        & Both &          4.24 &  \\textbf{\\uline{6.81}} &          4.17 &  0.94 &         3.18 &          4.06 &  3.30 &  0.91 \\\\\nRating & \\abr{nyt} &          4.40 &  \\textbf{\\uline{5.87}} &          3.85 &  3.93 &         3.97 &          4.44 &  4.03 &  3.89 \\\\\n        & \\abr{wiki} &  \\uline{4.84} &  \\textbf{\\uline{5.95}} &  \\uline{5.65} &  1.33 &         2.96 &          3.73 &  3.69 &  0.62 \\\\\n        & Both &          4.49 &  \\textbf{\\uline{5.80}} &          4.56 &  1.78 &         3.45 &          3.91 &  3.81 &  1.32 \\\\\n\\bottomrule\n\\end{tabular}\n\n"
     ]
    }
   ],
   "source": [
    "# make latex\n",
    "regressions_pivot_coefs = regressions_pivot[\"coef\"].loc[[\n",
    "    ('intrusions',  'nytimes'),\n",
    "    ('intrusions', 'wikitext'),\n",
    "    ('intrusions',      'all'),\n",
    "    (   'ratings',  'nytimes'),\n",
    "    (   'ratings', 'wikitext'),\n",
    "    (   'ratings',      'all'),\n",
    "]]\n",
    "latex = regressions_pivot_coefs.to_latex(escape=False, multicolumn_format='c', column_format=\"ll|rrrr|rrrr\")\n",
    "\n",
    "for to_replace, val in LATEX_CLEANUP.items():\n",
    "    latex = latex.replace(to_replace, val)\n",
    "print(latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}