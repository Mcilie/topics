{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd0cf30b2a248a022118eaf3990843f204490a3a4dad78b3cc35fb5102bf723d6fa",
   "display_name": "Python 3.8.10 64-bit ('statsmodels-dev': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, mannwhitneyu, ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path) as infile:\n",
    "        return json.load(infile)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, 'w') as outfile:\n",
    "        return json.dump(obj, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/human/all_data\"\n",
    "DATA_FILE = \"all_data.csv\"\n",
    "FILTER_ON_FAMILIARITY = False\n",
    "DROP_NA = False # if true, then nan/inf set to 0\n",
    "AVERAGE_ANNOTATIONS = False # average over human annotators before running regression\n",
    "ITERS = 1_000"
   ]
  },
  {
   "source": [
    "## Data compilation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_metric_names = [\n",
    " 'c_npmi_10_full',\n",
    " 'c_npmi_10_nytimes_full',\n",
    " #'c_npmi_10_test',\n",
    " 'c_npmi_10_train',\n",
    " 'c_npmi_10_val',\n",
    " 'c_npmi_10_wikitext_full',\n",
    " #'c_uci_full',\n",
    " 'c_v_full',\n",
    " 'c_v_nytimes_full',\n",
    " #'c_v_test',\n",
    " 'c_v_train',\n",
    " 'c_v_val',\n",
    " 'c_v_wikitext_full',\n",
    " #'u_mass_full'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data = pd.read_csv(Path(DATA_DIR, DATA_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the out-of-sample columns\n",
    "for dataset in [\"wikitext\", \"nytimes\"]:\n",
    "    for metric in [\"c_npmi_10\", \"c_v\"]:\n",
    "        task_data[f\"{metric}_{dataset}_full\"] = task_data[f\"{metric}_{dataset}_full\"].combine_first(task_data[f\"{metric}_full\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the constant for lin. reg\n",
    "task_data = sm.add_constant(task_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert infinite values to nans\n",
    "task_data = task_data.replace(np.inf, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER_ON_FAMILIARITY:\n",
    "    task_data = task_data.loc[task_data.confidences_raw == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_coefs(coefs, lbs, ubs, contiguous=False):\n",
    "    n = len(coefs)\n",
    "    coefs = np.nan_to_num(coefs, nan=0)\n",
    "    lbs = np.nan_to_num(lbs, nan=0)\n",
    "    ubs = np.nan_to_num(ubs, nan=0)\n",
    "    sorted_coef_idx = np.argsort(coefs)[::-1]\n",
    "    for i, idx in enumerate(sorted_coef_idx[1:], start=1):\n",
    "        # is the upper bound of this coefficient contained in the\n",
    "        # lower bound of the next-largest coefficient?\n",
    "        prev_idx = sorted_coef_idx[i-1] if contiguous else sorted_coef_idx[0]\n",
    "        if ubs[idx] < lbs[prev_idx]:\n",
    "            return np.array([j in sorted_coef_idx[:i] for j in range(n)])\n",
    "    return np.full(n, True)"
   ]
  },
  {
   "source": [
    "## Bootstrapped correlations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [01:17<00:00,  7.79s/it]\n"
     ]
    }
   ],
   "source": [
    "AVERAGE_ANNOTATIONS = False\n",
    "FILTER_ON_FAMILIARITY = False\n",
    "DROP_NA = False\n",
    "USE_OLS = False\n",
    "ALPHA = 0.05\n",
    "N_ITERS = 500\n",
    "NUM_ANNOTATORS = {\"intrusions\": 26, \"ratings\": 15}\n",
    "\n",
    "np.random.seed(42)\n",
    "rows = []\n",
    "for iteration in tqdm(range(N_ITERS), total=N_ITERS):\n",
    "    for task in [\"ratings\", \"intrusions\"]:\n",
    "        for dataset in [\"wikitext\", \"nytimes\", \"all\"]:\n",
    "            data_df = task_data.loc[task_data.task == task]\n",
    "            if dataset != \"all\":\n",
    "                data_df = data_df.loc[data_df.dataset == dataset]\n",
    "            if FILTER_ON_FAMILIARITY:\n",
    "                data_df = data_df.loc[data_df.confidences_raw == 1]\n",
    "\n",
    "            # run regressions for each metric\n",
    "            for i, metric in enumerate(auto_metric_names):\n",
    "                #print(f\"\\n===={task}, {dataset}, {metric}====\\n\")\n",
    "                if dataset in metric:\n",
    "                    continue # don't re-do the internal\n",
    "                if DROP_NA:\n",
    "                    df = data_df.dropna(subset=[metric])\n",
    "                else:\n",
    "                    df = data_df.fillna(0)\n",
    "                # sample \n",
    "                df = (\n",
    "                    df.groupby([\"task\", \"dataset\", \"model\", \"topic_idx\"])\n",
    "                    .sample(NUM_ANNOTATORS[task], replace=True)\n",
    "                    .groupby([\"task\", \"dataset\", \"model\", \"topic_idx\"])[[metric, \"scores_raw\"]]\n",
    "                    .mean()\n",
    "                )\n",
    "                spear_rho, spear_p = spearmanr(df[metric].values, df[\"scores_raw\"].values)\n",
    "                pear_rho, pear_p = pearsonr(df[metric].values, df[\"scores_raw\"].values)\n",
    "                metric_base = re.search(\"c_npmi_10|c_v|c_uci|u_mass\", metric).group(0)\n",
    "                row = {\n",
    "                    \"task\": task,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"metric\": metric_base,\n",
    "                    \"reference\": metric.replace(f\"{metric_base}_\", \"\"),\n",
    "                    \"spear_rho\": spear_rho,\n",
    "                    \"pear_rho\": pear_rho,\n",
    "                    \"spear_p\": spear_p,\n",
    "                    \"pear_p\": pear_p,\n",
    "                }\n",
    "                rows.append(row)\n",
    "correlations = pd.DataFrame(rows)\n",
    "correlations.to_csv(\"correlation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = pd.read_csv(\"correlation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations.loc[(correlations.dataset==\"wikitext\") & (correlations.reference == \"full\"), \"reference\"] = \"wikitext_full\"\n",
    "correlations.loc[(correlations.dataset==\"nytimes\") & (correlations.reference == \"full\"), \"reference\"] = \"nytimes_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_COL = \"spear_rho\"\n",
    "\n",
    "def ci_lb(x):\n",
    "    return np.quantile(x, 0.025)\n",
    "def ci_ub(x):\n",
    "    return np.quantile(x, 0.975)\n",
    "\n",
    "correlations_grouped = (\n",
    "    correlations.groupby([\"task\", \"dataset\", \"metric\", \"reference\"])[VALUE_COL]\n",
    "               .agg([\"mean\", \"std\", ci_lb, ci_ub])\n",
    "               .reset_index()\n",
    "               .rename(columns={\"mean\": VALUE_COL, \"ci_lb\": \"ci_0.025\", \"ci_ub\": \"ci_0.975\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_TO_KEEP = [\"c_v\", \"c_npmi_10\"]\n",
    "REPORT_SD = False\n",
    "REPORT_CI = not REPORT_SD\n",
    "correlations_pivot = (\n",
    "    correlations_grouped.loc[correlations_grouped.metric.isin(METRICS_TO_KEEP)]\n",
    "               .sort_values([\"task\", \"dataset\", \"metric\", \"reference\"])\n",
    "               #.pivot(index=[\"Metric\", \"Reference\"], columns=[\"Task\", \"Dataset\"], values=\"coef\")\n",
    "               .pivot(index=[\"task\", \"dataset\"], columns=[\"metric\", \"reference\"], values=[VALUE_COL, \"std\", \"ci_0.025\",  \"ci_0.975\"])\n",
    ")\n",
    "correlations_pivot = correlations_pivot[[\n",
    "    (v, m, r)\n",
    "    for v in [VALUE_COL, \"std\", \"ci_0.025\",  \"ci_0.975\"]\n",
    "    for m in [\"c_npmi_10\", \"c_v\"]\n",
    "    for r in [\"nytimes_full\", \"wikitext_full\", \"train\", \"val\"] #NB: val excluded\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPORT_SD:\n",
    "    # bold format the significant values\n",
    "    for idx, row in correlations_pivot.iterrows():\n",
    "        newrow = []\n",
    "        for i, x in enumerate(row[VALUE_COL]):\n",
    "            if np.isnan(x):\n",
    "                val = \"-\"\n",
    "            else:\n",
    "                std = row[\"std\"][i]\n",
    "                val = f\"${x:0.3f}_{{{std:0.3f}}}$\"\n",
    "                if x == row.max():\n",
    "                    val = r\"\\textbf{\" + val + \"}\"\n",
    "            newrow.append(val)\n",
    "        correlations_pivot.at[idx, VALUE_COL] = newrow\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPORT_CI:\n",
    "    # bold format the significant values\n",
    "    for idx, row in correlations_pivot.iterrows():\n",
    "        max_coefs = find_max_coefs(row[VALUE_COL], row[\"ci_0.025\"], row[\"ci_0.975\"])\n",
    "        newrow = []\n",
    "        for i, x in enumerate(row[VALUE_COL]):\n",
    "            if np.isnan(x):\n",
    "                val = \"-\"\n",
    "            elif not np.isnan(x) and max_coefs[i]:\n",
    "                val = r\"\\uline{\" + f\"{x:0.2f}\" + \"}\"\n",
    "                if x == np.max(row[VALUE_COL]):\n",
    "                    val = r\"\\textbf{\" + val + \"}\"\n",
    "            else:\n",
    "                val = f\"{x:0.2f}\"\n",
    "            newrow.append(val)\n",
    "        correlations_pivot.at[idx, VALUE_COL] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\begin{tabular}{ll|rrrr|rrrr}\n\\toprule\n        &  & \\multicolumn{4}{c}{\\abr{npmi} (10-token window)} & \\multicolumn{4}{c}{$C_v$ (110-token window)} \\\\\n        & Ref. Corpus $\\rightarrow$ &  \\abr{nyt} &          \\abr{wiki} &                  Train &   Val &  \\abr{nyt} &          \\abr{wiki} &         Train &           Val \\\\\n & Train Corpus $\\downarrow$ &               &                        &                        &       &               &                        &               &               \\\\\n\\midrule\nIntrusion & \\abr{nyt} &          0.27 &           \\uline{0.43} &                   0.27 &  0.24 &          0.34 &  \\textbf{\\uline{0.45}} &          0.35 &          0.34 \\\\\n        & \\abr{wiki} &  \\uline{0.34} &           \\uline{0.36} &  \\textbf{\\uline{0.39}} &  0.17 &  \\uline{0.31} &           \\uline{0.34} &  \\uline{0.34} &          0.20 \\\\\n        & Both &          0.29 &           \\uline{0.40} &                   0.32 &  0.17 &          0.32 &  \\textbf{\\uline{0.40}} &  \\uline{0.35} &          0.24 \\\\\nRating & \\abr{nyt} &          0.37 &  \\textbf{\\uline{0.48}} &                   0.38 &  0.39 &  \\uline{0.41} &           \\uline{0.46} &  \\uline{0.44} &  \\uline{0.45} \\\\\n        & \\abr{wiki} &          0.33 &           \\uline{0.41} &  \\textbf{\\uline{0.44}} &  0.28 &          0.32 &           \\uline{0.40} &  \\uline{0.41} &  \\uline{0.34} \\\\\n        & Both &          0.37 &  \\textbf{\\uline{0.44}} &           \\uline{0.41} &  0.35 &          0.38 &           \\uline{0.42} &  \\uline{0.42} &  \\uline{0.42} \\\\\n\\bottomrule\n\\end{tabular}\n\n"
     ]
    }
   ],
   "source": [
    "# make latex\n",
    "correlations_pivot_values = correlations_pivot[VALUE_COL].loc[[\n",
    "    ('intrusions',  'nytimes'),\n",
    "    ('intrusions', 'wikitext'),\n",
    "    ('intrusions',      'all'),\n",
    "    (   'ratings',  'nytimes'),\n",
    "    (   'ratings', 'wikitext'),\n",
    "    (   'ratings',      'all'),\n",
    "]]\n",
    "latex = correlations_pivot_values.to_latex(escape=False, multicolumn_format='c', column_format=\"ll|rrrr|rrrr\")\n",
    "to_replace_in_latex={\n",
    "    \"c_v\": r\"$C_v$ (110-token window)\",\n",
    "    \"test\": \"Test\",\n",
    "    \"c_npmi_10\": r\"\\abr{npmi} (10-token window)\",\n",
    "    \"nytimes_full\": r\"\\abr{nyt}\",\n",
    "    \"wikitext_full\": r\"\\abr{wiki}\",\n",
    "    \"wikitext\": r\"\\abr{wiki}\",\n",
    "    \"nytimes\": r\"\\abr{nyt}\",\n",
    "    \"all\": \"Both\",\n",
    "    \"full\": \"Full\",\n",
    "    \"train\": \"Train\",\n",
    "    \"val\": \"Val\",\n",
    "    \"test\": \"Test\",\n",
    "    \"ratings\": \"Rating\",\n",
    "    \"intrusions\": \"Intrusion\",\n",
    "    \"metric\": \"\",\n",
    "    \"dataset\": r\"Train Corpus $\\downarrow$\",\n",
    "    \"reference\": r\"Ref. Corpus $\\rightarrow$\",\n",
    "    \"task\": \"\",\n",
    "}\n",
    "for to_replace, val in to_replace_in_latex.items():\n",
    "    latex = latex.replace(to_replace, val)\n",
    "print(latex)\n"
   ]
  },
  {
   "source": [
    "# Simple Simulation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_topics(df, num_topics=50):\n",
    "    topic_sample = (\n",
    "        df.groupby([\"task\", \"dataset\", \"model\", \"topic_idx\"])\n",
    "                    .size()\n",
    "                    .reset_index()\n",
    "                    .groupby([\"task\", \"dataset\", \"model\"])\n",
    "                    .sample(num_topics, replace=True)\n",
    "                    .drop(columns=0)\n",
    "    )\n",
    "    topic_sample[\"rand_topic_idx\"] = topic_sample.groupby(['task', 'dataset', 'model']).cumcount()\n",
    "    df = df.merge(topic_sample, how='inner')\n",
    "    return df\n",
    "\n",
    "def intrusion_test(scores_a, scores_b, alternative=\"larger\"):\n",
    "    return proportions_ztest(\n",
    "        [scores_a.sum(), scores_b.sum()],\n",
    "        [len(scores_a), len(scores_b)],\n",
    "        alternative=alternative\n",
    "    )\n",
    "\n",
    "def ratings_test(scores_a, scores_b, alternative=\"greater\"):\n",
    "    return mannwhitneyu(scores_a, scores_b, alternative=alternative)\n",
    "\n",
    "def auto_test(scores_a, scores_b, alternative=\"greater\"):\n",
    "    return ttest_ind(scores_a, scores_b, equal_var=False, alternative=alternative)\n",
    "\n",
    "def false_discovery_rate_sim(\n",
    "    task,\n",
    "    test_df,\n",
    "    metric,\n",
    "    alpha=0.05,\n",
    "    beta=0.1,\n",
    "    models=[\"mallet\", \"dvae\", \"etm\"],\n",
    "):\n",
    "    false_positives, false_negatives = 0, 0\n",
    "    discoveries, omissions = 0, 0\n",
    "    total = 0\n",
    "    if task == \"intrusions\":\n",
    "        stat_test = intrusion_test\n",
    "    elif task == \"ratings\":\n",
    "        stat_test = ratings_test\n",
    "    model_scores = test_df.groupby(['task', 'dataset', 'model', 'rand_topic_idx']).head(1)\n",
    "\n",
    "    # Calculate the false discovery rate\n",
    "    for model_a, model_b in itertools.permutations(models, 2):\n",
    "        model_a_idxr = test_df.model==model_a\n",
    "        model_b_idxr = test_df.model==model_b\n",
    "        # run the tests for both human and auto metrics\n",
    "        stat_auto, p_auto = auto_test(\n",
    "            model_scores.loc[model_scores.model==model_a][metric],\n",
    "            model_scores.loc[model_scores.model==model_b][metric],\n",
    "        )\n",
    "        stat_human, p_human = stat_test(\n",
    "            test_df.loc[test_df.model==model_a][\"scores_raw\"],\n",
    "            test_df.loc[test_df.model==model_b][\"scores_raw\"],\n",
    "        )\n",
    "\n",
    "        if p_auto < alpha: # auto rejects the null\n",
    "            if np.random.random() < alpha:\n",
    "                continue # falsely rejected the null, type I error\n",
    "            discoveries += 1\n",
    "            if np.random.random() < beta: # human failed to detect effect. type II error\n",
    "                continue\n",
    "            false_positives += p_human > alpha # human fails to reject the null\n",
    "\n",
    "        if p_human < alpha: # human rejects the null\n",
    "            if np.random.random() < alpha: # falsely rejected the null, type I error\n",
    "                continue\n",
    "            omissions += 1\n",
    "            if np.random.random() < beta: # model failed to detect effect. type II error\n",
    "                continue\n",
    "            false_negatives += p_auto > alpha # auto fails to reject null\n",
    "        \n",
    "        # TODO: false negatives\n",
    "        total += 1\n",
    "    false_pos_rate = np.nan if discoveries == 0 else false_positives / discoveries\n",
    "    false_neg_rate = np.nan if omissions == 0 else false_negatives / omissions\n",
    "    return false_pos_rate, false_positives, discoveries, false_neg_rate, false_negatives, omissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 50/50 [01:42<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "AVERAGE_ANNOTATIONS = False\n",
    "FILTER_ON_FAMILIARITY = False\n",
    "DROP_NA = False\n",
    "USE_OLS = False\n",
    "ALPHA = 0.05\n",
    "BETA = 0.1\n",
    "N_ITERS = 1000\n",
    "NUM_TOPICS = 50\n",
    "MODELS = ['mallet', 'dvae', 'etm']\n",
    "\n",
    "np.random.seed(42)\n",
    "rows = []\n",
    "for iteration in tqdm(range(N_ITERS), total=N_ITERS):\n",
    "    for task in [\"ratings\", \"intrusions\"]:\n",
    "        for dataset in [\"wikitext\", \"nytimes\", \"all\"]:\n",
    "            data_df = task_data.loc[task_data.task == task]\n",
    "            if dataset != \"all\":\n",
    "                data_df = data_df.loc[data_df.dataset == dataset]\n",
    "            if FILTER_ON_FAMILIARITY:\n",
    "                data_df = data_df.loc[data_df.confidences_raw == 1]\n",
    "\n",
    "            # run regressions for each metric\n",
    "            for i, metric in enumerate(auto_metric_names):\n",
    "                #print(f\"\\n===={task}, {dataset}, {metric}====\\n\")\n",
    "                if dataset in metric:\n",
    "                    continue # don't re-do the internal\n",
    "                if DROP_NA:\n",
    "                    df = data_df.dropna(subset=[metric])\n",
    "                else:\n",
    "                    df = data_df.fillna(0)\n",
    "                # sample at the topic level\n",
    "                df = select_random_topics(df, NUM_TOPICS)\n",
    "                fp_rate, fp, pos, fn_rate, fn, neg = false_discovery_rate_sim(\n",
    "                    task,\n",
    "                    df,\n",
    "                    metric,\n",
    "                    alpha=ALPHA,\n",
    "                    beta=BETA,\n",
    "                )\n",
    "                metric_base = re.search(\"c_npmi_10|c_v|c_uci|u_mass\", metric).group(0)\n",
    "                row = {\n",
    "                    \"task\": task,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"metric\": metric_base,\n",
    "                    \"reference\": metric.replace(f\"{metric_base}_\", \"\"),\n",
    "                    \"fp_rate\": fp_rate,\n",
    "                    \"false_positives\": fp,\n",
    "                    \"discoveries\": pos,\n",
    "                    \"fn_rate\": fn_rate,\n",
    "                    \"false_negatives\": fn,\n",
    "                    \"omissions\": neg,\n",
    "                }\n",
    "                rows.append(row)\n",
    "simulations = pd.DataFrame(rows)\n",
    "#simulations.to_csv(\"simple_simulation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = pd.read_csv(\"simple_simulation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations.loc[(simulations.dataset==\"wikitext\") & (simulations.reference == \"full\"), \"reference\"] = \"wikitext_full\"\n",
    "simulations.loc[(simulations.dataset==\"nytimes\") & (simulations.reference == \"full\"), \"reference\"] = \"nytimes_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT = \"fnr\"\n",
    "\n",
    "def ci_lb(x):\n",
    "    return np.quantile(x, 0.025)\n",
    "def ci_ub(x):\n",
    "    return np.quantile(x, 0.975)\n",
    "\n",
    "simulations_grouped = (\n",
    "    simulations.groupby([\"task\", \"dataset\", \"metric\", \"reference\"])[[\"false_positives\", \"discoveries\", 'false_negatives', 'omissions']]\n",
    "               .agg([\"sum\"])\n",
    "               .reset_index()\n",
    ")\n",
    "simulations_grouped[\"fp_rate\"] = simulations_grouped[\"false_positives\"] / simulations_grouped[\"discoveries\"]\n",
    "simulations_grouped[\"fn_rate\"] = simulations_grouped[\"false_negatives\"] / simulations_grouped[\"omissions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_TO_KEEP = [\"c_v\", \"c_npmi_10\"]\n",
    "simulations_pivot = (\n",
    "    simulations_grouped.loc[simulations_grouped.metric.isin(METRICS_TO_KEEP)]\n",
    "               .sort_values([\"task\", \"dataset\", \"metric\", \"reference\"])\n",
    "               #.pivot(index=[\"Metric\", \"Reference\"], columns=[\"Task\", \"Dataset\"], values=\"coef\")\n",
    "               .pivot(index=[\"task\", \"dataset\"], columns=[\"metric\", \"reference\"], values=[VALUE_COL, \"fn_rate\"])#, \"std\", \"ci_0.025\",  \"ci_0.975\"])\n",
    ")\n",
    "simulations_pivot = simulations_pivot[[\n",
    "    (v, m, r)\n",
    "    for v in [VALUE_COL, \"fn_rate\"]#, \"std\", \"ci_0.025\",  \"ci_0.975\"]\n",
    "    for m in [\"c_npmi_10\", \"c_v\"]\n",
    "    for r in [\"nytimes_full\", \"wikitext_full\", \"train\"] #NB: test, val excluded\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPORT == \"fnr\":\n",
    "    # bold format the significant values\n",
    "    for idx, row in simulations_pivot.iterrows():\n",
    "        newrow = []\n",
    "        prec = 1 - row[\"fp_rate\"]\n",
    "        rec = 1 - row[\"fn_rate\"]\n",
    "        f1s = 2 * ((prec * rec) / (prec + rec))\n",
    "        for i, (fpr, fnr, f1) in enumerate(zip(row[\"fp_rate\"], row[\"fn_rate\"], f1s)):\n",
    "            if np.isnan(fpr):\n",
    "                val = \"-\"\n",
    "            else:\n",
    "                val = f\"{fpr*100:0.0f}\"\n",
    "                if f1 == np.max(f1s):\n",
    "                    val = r\"\\textbf{\" + val + \"}\"\n",
    "                if len(val) < 2:\n",
    "                    val = r\"\\phantom{0}\" + val\n",
    "                fnr_str = f\"{fnr*100:0.0f}\"\n",
    "                if len(fnr_str) < 2:\n",
    "                    fnr_str = r\"\\phantom{0}\" + fnr_str\n",
    "                val += r\" / \\textcolor{gray}{\" + fnr_str + \"}\"\n",
    "            newrow.append(val)\n",
    "        simulations_pivot.at[idx, \"fp_rate\"] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPORT == \"sd\":\n",
    "    # bold format the significant values\n",
    "    for idx, row in simulations_pivot.iterrows():\n",
    "        newrow = []\n",
    "        for i, x in enumerate(row[VALUE_COL]):\n",
    "            if np.isnan(x):\n",
    "                val = \"-\"\n",
    "            else:\n",
    "                #std = row[\"std\"][i]\n",
    "                #val = f\"${x*100:0.0f}_{{{std*100:0.0fx}}}$\"\n",
    "                val = f\"{x*100:0.1f}\"\n",
    "                if x == np.min(row[VALUE_COL]):\n",
    "                    val = r\"\\textbf{\" + val + \"}\"\n",
    "            newrow.append(val)\n",
    "        simulations_pivot.at[idx, VALUE_COL] = newrow\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPORT == \"ci\":\n",
    "    # bold format the significant values\n",
    "    for idx, row in simulations_pivot.iterrows():\n",
    "        max_coefs = find_max_coefs(row[VALUE_COL], row[\"ci_0.025\"], row[\"ci_0.975\"])\n",
    "        newrow = []\n",
    "        for i, x in enumerate(row[VALUE_COL]):\n",
    "            if np.isnan(x):\n",
    "                val = \"-\"\n",
    "            elif not np.isnan(x) and max_coefs[i]:\n",
    "                val = r\"\\uline{\" + f\"{x:0.2f}\" + \"}\"\n",
    "                if x == np.min(row[VALUE_COL]):\n",
    "                    val = r\"\\textbf{\" + val + \"}\"\n",
    "            else:\n",
    "                val = f\"{x:0.2f}\"\n",
    "            newrow.append(val)\n",
    "        simulations_pivot.at[idx, VALUE_COL] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\begin{tabular}{ll|rrr|rrr}\n\\toprule\n        &  & \\multicolumn{3}{c}{\\abr{npmi} (10-token window)} & \\multicolumn{3}{c}{$C_v$ (110-token window)} \\\\\n        & Ref. $\\rightarrow$ &               \\abr{nyt} &                       \\abr{wiki} &                                Train &                                  \\abr{nyt} &                       \\abr{wiki} &                                         Train \\\\\n & Train  $\\downarrow$ &                            &                                     &                                      &                                               &                                     &                                               \\\\\n\\midrule\nIntrusion & \\abr{nyt} &  21 / \\textcolor{gray}{10} &           65 / \\textcolor{gray}{70} &            19 / \\textcolor{gray}{11} &  \\textbf{20} / \\textcolor{gray}{\\phantom{0}5} &           75 / \\textcolor{gray}{78} &           21 / \\textcolor{gray}{\\phantom{0}5} \\\\\n        & \\abr{wiki} &  77 / \\textcolor{gray}{75} &  \\textbf{35} / \\textcolor{gray}{17} &            36 / \\textcolor{gray}{23} &                     81 / \\textcolor{gray}{80} &           36 / \\textcolor{gray}{19} &                     38 / \\textcolor{gray}{18} \\\\\n        & Both &  79 / \\textcolor{gray}{84} &           75 / \\textcolor{gray}{82} &  18 / \\textcolor{gray}{\\phantom{0}5} &                     88 / \\textcolor{gray}{88} &           89 / \\textcolor{gray}{89} &  \\textbf{18} / \\textcolor{gray}{\\phantom{0}3} \\\\\nRating & \\abr{nyt} &  43 / \\textcolor{gray}{44} &  \\textbf{24} / \\textcolor{gray}{42} &            44 / \\textcolor{gray}{45} &                     43 / \\textcolor{gray}{40} &           37 / \\textcolor{gray}{47} &                     42 / \\textcolor{gray}{40} \\\\\n        & \\abr{wiki} &  46 / \\textcolor{gray}{48} &           43 / \\textcolor{gray}{44} &            41 / \\textcolor{gray}{44} &                     53 / \\textcolor{gray}{60} &  \\textbf{41} / \\textcolor{gray}{43} &                     42 / \\textcolor{gray}{43} \\\\\n        & Both &  39 / \\textcolor{gray}{54} &           27 / \\textcolor{gray}{58} &            37 / \\textcolor{gray}{38} &                     47 / \\textcolor{gray}{63} &           47 / \\textcolor{gray}{62} &            \\textbf{35} / \\textcolor{gray}{36} \\\\\n\\bottomrule\n\\end{tabular}\n\n"
     ]
    }
   ],
   "source": [
    "# make latex\n",
    "simulations_pivot_values = simulations_pivot[VALUE_COL].loc[[\n",
    "    ('intrusions',  'nytimes'),\n",
    "    ('intrusions', 'wikitext'),\n",
    "    ('intrusions',      'all'),\n",
    "    (   'ratings',  'nytimes'),\n",
    "    (   'ratings', 'wikitext'),\n",
    "    (   'ratings',      'all'),\n",
    "]]\n",
    "latex = simulations_pivot_values.to_latex(escape=False, multicolumn_format='c', column_format=\"ll|rrr|rrr\")\n",
    "to_replace_in_latex={\n",
    "    \"c_v\": r\"$C_v$ (110-token window)\",\n",
    "    \"test\": \"Test\",\n",
    "    \"c_npmi_10\": r\"\\abr{npmi} (10-token window)\",\n",
    "    \"nytimes_full\": r\"\\abr{nyt}\",\n",
    "    \"wikitext_full\": r\"\\abr{wiki}\",\n",
    "    \"wikitext\": r\"\\abr{wiki}\",\n",
    "    \"nytimes\": r\"\\abr{nyt}\",\n",
    "    \"all\": \"Both\",\n",
    "    \"full\": \"Full\",\n",
    "    \"train\": \"Train\",\n",
    "    \"test\": \"Test\",\n",
    "    \"val\": \"Val\",\n",
    "    \"ratings\": \"Rating\",\n",
    "    \"intrusions\": \"Intrusion\",\n",
    "    \"metric\": \"\",\n",
    "    \"dataset\": r\"Train  $\\downarrow$\",\n",
    "    \"reference\": r\"Ref. $\\rightarrow$\",\n",
    "    \"task\": \"\",\n",
    "}\n",
    "for to_replace, val in to_replace_in_latex.items():\n",
    "    latex = latex.replace(to_replace, val)\n",
    "print(latex)\n"
   ]
  },
  {
   "source": [
    "## Regression "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Assess variances by topic. This may help determine if averaging is ok"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stds = task_data.groupby(['task', 'dataset', 'model', 'topic_idx']).agg({\"scores_raw\": \"std\"}).reset_index()\n",
    "topic_stds[\"scores_raw\"].hist(by=topic_stds[\"task\"])"
   ]
  },
  {
   "source": [
    " First, simple linear regression (ignoring model effects)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AVERAGE_ANNOTATIONS = False\n",
    "FILTER_ON_FAMILIARITY = False\n",
    "DROP_NA = False\n",
    "USE_OLS = False\n",
    "ALPHA = 0.05\n",
    "np.random.seed(42)\n",
    "rows = []\n",
    "for task in [\"ratings\", \"intrusions\"]:\n",
    "    for dataset in [\"wikitext\", \"nytimes\", \"all\"]:\n",
    "        data_df = task_data.loc[task_data.task == task]\n",
    "        if dataset != \"all\":\n",
    "            data_df = data_df.loc[data_df.dataset == dataset]\n",
    "        if FILTER_ON_FAMILIARITY:\n",
    "            data_df = data_df.loc[data_df.confidences_raw == 1]\n",
    "\n",
    "        # run regressions for each metric\n",
    "        for i, metric in enumerate(auto_metric_names):\n",
    "            #print(f\"\\n===={task}, {dataset}, {metric}====\\n\")\n",
    "            if dataset in metric:\n",
    "                continue # don't re-do the internal\n",
    "            if DROP_NA:\n",
    "                df = data_df.dropna(subset=[metric])\n",
    "            else:\n",
    "                df = data_df.fillna(0)\n",
    "            if AVERAGE_ANNOTATIONS:\n",
    "                df = df.groupby([\"model\", \"topic_idx\"]).mean().reset_index()\n",
    "                mod = sm.OLS(df[\"scores_raw\"], df[[\"const\", metric]])\n",
    "            if USE_OLS:\n",
    "                scores = df[\"scores_raw\"] if task == \"intrusions\" else (df[\"scores_raw\"] - 1) / 2\n",
    "                mod = sm.OLS(scores, df[[\"const\", metric]])\n",
    "            elif task == \"intrusions\":\n",
    "                mod = sm.Logit(df[\"scores_raw\"], df[[\"const\", metric]])\n",
    "            elif task == \"ratings\":\n",
    "                mod = OrderedModel(df[\"scores_raw\"], df[[metric]], distr=\"probit\")\n",
    "            res = mod.fit(disp=0)\n",
    "            metric_base = re.search(\"c_npmi_10|c_v|c_uci|u_mass\", metric).group(0)\n",
    "            ci_lb, ci_ub = res.conf_int(alpha=ALPHA).loc[metric] \n",
    "            row = {\n",
    "                \"task\": task,\n",
    "                \"dataset\": dataset,\n",
    "                \"metric\": metric_base,\n",
    "                \"reference\": metric.replace(f\"{metric_base}_\", \"\"),\n",
    "                \"coef\": res.params[metric],\n",
    "                \"se\": res.bse[metric],\n",
    "                \"p\": res.pvalues[metric],\n",
    "                \"bic\": res.bic,\n",
    "                \"ci_0.025\": ci_lb,\n",
    "                \"ci_0.975\": ci_ub,\n",
    "            }\n",
    "            rows.append(row)\n",
    "regressions = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FILTER_ON_FAMILIARITY and not AVERAGE_ANNOTATIONS:\n",
    "    regressions.to_csv(\"regression_results.csv\", index=False)"
   ]
  },
  {
   "source": [
    "### Make the latex table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We want to bold the values that are significantly larger than others using the CIs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_coefs(coefs, lbs, ubs, contiguous=False):\n",
    "    n = len(coefs)\n",
    "    coefs = np.nan_to_num(coefs, nan=0)\n",
    "    lbs = np.nan_to_num(lbs, nan=0)\n",
    "    ubs = np.nan_to_num(ubs, nan=0)\n",
    "    sorted_coef_idx = np.argsort(coefs)[::-1]\n",
    "    for i, idx in enumerate(sorted_coef_idx[1:], start=1):\n",
    "        # is the upper bound of this coefficient contained in the\n",
    "        # lower bound of the next-largest coefficient?\n",
    "        prev_idx = sorted_coef_idx[i-1] if contiguous else sorted_coef_idx[0]\n",
    "        if ubs[idx] < lbs[prev_idx]:\n",
    "            return np.array([j in sorted_coef_idx[:i] for j in range(n)])\n",
    "    return np.full(n, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_TO_KEEP = [\"c_v\", \"c_npmi_10\"]\n",
    "regressions.loc[(regressions.dataset==\"wikitext\") & (regressions.reference == \"full\"), \"reference\"] = \"wikitext_full\"\n",
    "regressions.loc[(regressions.dataset==\"nytimes\") & (regressions.reference == \"full\"), \"reference\"] = \"nytimes_full\"\n",
    "regressions_pivot = (\n",
    "    regressions.loc[regressions.metric.isin(METRICS_TO_KEEP)]\n",
    "               .replace({\"\"})\n",
    "               .sort_values([\"task\", \"dataset\", \"metric\", \"reference\"])\n",
    "               #.pivot(index=[\"Metric\", \"Reference\"], columns=[\"Task\", \"Dataset\"], values=\"coef\")\n",
    "               .pivot(index=[\"task\", \"dataset\"], columns=[\"metric\", \"reference\"], values=[\"coef\", \"ci_0.025\", \"ci_0.975\"])\n",
    ")\n",
    "# order the columns\n",
    "regressions_pivot = regressions_pivot[[\n",
    "    (v, m, r)\n",
    "    for v in [\"coef\", \"ci_0.025\", \"ci_0.975\"]\n",
    "    for m in [\"c_npmi_10\", \"c_v\"]\n",
    "    for r in [\"nytimes_full\", \"wikitext_full\", \"train\", \"val\"] #NB: test, full excluded\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bold format the significant values\n",
    "for idx, row in regressions_pivot.iterrows():\n",
    "    max_coefs = find_max_coefs(row['coef'], row[\"ci_0.025\"], row[\"ci_0.975\"])\n",
    "    newrow = []\n",
    "    for i, x in enumerate(row[\"coef\"]):\n",
    "        if np.isnan(x):\n",
    "            val = \"-\"\n",
    "        elif not np.isnan(x) and max_coefs[i]:\n",
    "            val = r\"\\uline{\" + f\"{x:0.2f}\" + \"}\"\n",
    "            if x == np.max(row['coef']):\n",
    "                val = r\"\\textbf{\" + val + \"}\"\n",
    "        else:\n",
    "            val = f\"{x:0.2f}\"\n",
    "        newrow.append(val)\n",
    "    regressions_pivot.at[idx, \"coef\"] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make latex\n",
    "regressions_pivot_coefs = regressions_pivot[\"coef\"].loc[[\n",
    "    ('intrusions',  'nytimes'),\n",
    "    ('intrusions', 'wikitext'),\n",
    "    ('intrusions',      'all'),\n",
    "    (   'ratings',  'nytimes'),\n",
    "    (   'ratings', 'wikitext'),\n",
    "    (   'ratings',      'all'),\n",
    "]]\n",
    "latex = regressions_pivot_coefs.to_latex(escape=False, multicolumn_format='c', column_format=\"ll|rrrr|rrrr\")\n",
    "to_replace_in_latex={\n",
    "    \"c_v\": r\"$C_v$ (110-token window)\",\n",
    "    \"test\": \"Test\",\n",
    "    \"c_npmi_10\": r\"\\abr{npmi} (10-token window)\",\n",
    "    \"nytimes_full\": r\"\\abr{nyt}\",\n",
    "    \"wikitext_full\": r\"\\abr{wiki}\",\n",
    "    \"wikitext\": r\"\\abr{wiki}\",\n",
    "    \"nytimes\": r\"\\abr{nyt}\",\n",
    "    \"all\": \"Both\",\n",
    "    \"full\": \"Full\",\n",
    "    \"train\": \"Train\",\n",
    "    \"val\": \"Val\",\n",
    "    \"ratings\": \"Rating\",\n",
    "    \"intrusions\": \"Intrusion\",\n",
    "    \"metric\": \"\",\n",
    "    \"dataset\": r\"Train Corpus $\\downarrow$\",\n",
    "    \"reference\": r\"Ref. Corpus $\\rightarrow$\",\n",
    "    \"task\": \"\",\n",
    "}\n",
    "for to_replace, val in to_replace_in_latex.items():\n",
    "    latex = latex.replace(to_replace, val)\n",
    "print(latex)\n"
   ]
  },
  {
   "source": [
    "Linear regression with model effects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Linear regression with familiarity effect"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Best explanation of data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Prediction Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(task_data, test_size):\n",
    "    topic_sample = (\n",
    "    task_data.groupby([\"task\", \"dataset\", \"model\", \"topic_idx\"])\n",
    "                .size()\n",
    "                .reset_index()\n",
    "                .groupby([\"task\", \"dataset\", \"model\"])\n",
    "                .sample(frac=1-test_size)\n",
    "                .drop(columns=0)\n",
    "    )\n",
    "    topic_sample\n",
    "    task_data[\"idx\"] = np.arange(len(task_data))\n",
    "    task_data_train = task_data.merge(topic_sample, how='inner')\n",
    "    task_data_test = task_data.loc[~task_data.idx.isin(task_data_train.idx)]\n",
    "    return task_data_train, task_data_test\n",
    "\n",
    "def random_choice_prob_index(a, axis=1):\n",
    "    # basically vectorized categorical draw https://stackoverflow.com/a/47722393\n",
    "    r = np.expand_dims(np.random.rand(a.shape[1-axis]), axis=axis)\n",
    "    return (a.cumsum(axis=axis) > r).argmax(axis=axis)\n",
    "\n",
    "def fit_predict_intrusion(train_df, test_df, metric):\n",
    "    exog_cols = [\"const\", metric] if isinstance(metric, str) else [\"const\"] + metric\n",
    "    mod = sm.Logit(train_df[\"scores_raw\"], train_df[exog_cols])\n",
    "    res = mod.fit(disp=0)\n",
    "    return res.model.predict(res.params, exog=test_df[exog_cols])\n",
    "\n",
    "def fit_predict_ratings(train_df, test_df, metric):\n",
    "    exog_cols = [metric] if isinstance(metric, str) else metric\n",
    "    mod = OrderedModel(train_df[\"scores_raw\"], train_df[exog_cols], distr=\"probit\")\n",
    "    res = mod.fit(method='bfgs', disp=0)\n",
    "    return res.model.predict(res.params, exog=test_df[exog_cols])\n",
    "\n",
    "def intrusion_test(scores_a, scores_b, alternative=\"larger\"):\n",
    "    return proportions_ztest(\n",
    "        [scores_a.sum(), scores_b.sum()],\n",
    "        [len(scores_a), len(scores_b)],\n",
    "        alternative=alternative\n",
    "    )\n",
    "\n",
    "def ratings_test(scores_a, scores_b, alternative=\"greater\"):\n",
    "    return mannwhitneyu(scores_a, scores_b, alternative=alternative)\n",
    "\n",
    "def false_discovery_rate_sim(\n",
    "    task,\n",
    "    test_df,\n",
    "    probs,\n",
    "    n_iters=100,\n",
    "    alpha=0.05,\n",
    "    beta=0.1,\n",
    "    models=[\"mallet\", \"dvae\", \"etm\"],\n",
    "):\n",
    "    false_positives, false_negatives = 0, 0\n",
    "    discoveries, omissions = 0, 0\n",
    "    total = 0\n",
    "    if task == \"intrusions\":\n",
    "        stat_test = intrusion_test\n",
    "    elif task == \"ratings\":\n",
    "        stat_test = ratings_test\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        # sample pseduo human scores\n",
    "        if task == \"intrusions\":\n",
    "            preds = np.random.binomial(1, p=probs)\n",
    "        elif task == \"ratings\":\n",
    "            preds = random_choice_prob_index(probs)\n",
    "\n",
    "        # Calculate the false discovery rate\n",
    "        for model_a, model_b in itertools.permutations(models, 2):\n",
    "            model_a_idxr = test_df.model==model_a\n",
    "            model_b_idxr = test_df.model==model_b\n",
    "            # run the tests for both human and auto metrics\n",
    "            stat_auto, p_auto = stat_test(preds[model_a_idxr], preds[model_b_idxr])\n",
    "            stat_human, p_human = stat_test(test_df.loc[model_a_idxr][\"scores_raw\"], test_df.loc[model_b_idxr][\"scores_raw\"])\n",
    "\n",
    "            if p_auto < alpha: # auto rejects the null\n",
    "                if np.random.random() < alpha:\n",
    "                    continue # falsely rejected the null, type I error\n",
    "                discoveries += 1\n",
    "                if np.random.random() < beta: # human failed to detect effect. type II error\n",
    "                    continue\n",
    "                false_positives += p_human > alpha # human fails to reject the null\n",
    "\n",
    "            # TODO: this is wrong, need to correct\n",
    "            if p_human < alpha: # human rejects the null\n",
    "                false_negatives += p_auto > alpha # auto fails to reject null\n",
    "                omissions += 1\n",
    "            total += 1\n",
    "    false_pos_rate = np.nan if discoveries == 0 else false_positives / discoveries\n",
    "    false_neg_rate = np.nan if omissions == 0 else false_negatives / omissions\n",
    "    return false_pos_rate, discoveries, false_neg_rate, omissions, (total - (false_positives + false_negatives)) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_ITERS = 50\n",
    "LOCAL_ITERS = 50\n",
    "TEST_SIZE = 0.5\n",
    "FILTER_ON_FAMILIARITY = False\n",
    "DROP_NA = False\n",
    "ALPHA = 0.05\n",
    "BETA = 0.1\n",
    "NUM_ANNOTATORS = {\"intrusions\": 26, \"ratings\": 15}\n",
    "METRICS_TO_USE = [\n",
    " 'c_npmi_10_full',\n",
    " 'c_npmi_10_nytimes_full',\n",
    " #'c_npmi_10_test',\n",
    " 'c_npmi_10_train',\n",
    " 'c_npmi_10_val',\n",
    " 'c_npmi_10_wikitext_full',\n",
    " 'c_v_full',\n",
    " 'c_v_nytimes_full',\n",
    " #'c_v_test',\n",
    " 'c_v_train',\n",
    " 'c_v_val',\n",
    " 'c_v_wikitext_full',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DROP_NA:\n",
    "    task_data = task_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "np.random.seed(42)\n",
    "for iteration in tqdm(range(GLOBAL_ITERS), total=GLOBAL_ITERS):\n",
    "    # iterate through variants and estimate models\n",
    "    task_data_train, task_data_test = create_split(task_data, TEST_SIZE)\n",
    "    \n",
    "    for task in [\"intrusions\", \"ratings\"]:\n",
    "        for dataset in [\"wikitext\", \"nytimes\", \"all\"]:\n",
    "            train_df = task_data_train.loc[task_data_train.task == task]\n",
    "            test_df = task_data_test.loc[task_data_test.task == task]\n",
    "\n",
    "            if dataset != \"all\":\n",
    "                train_df = train_df.loc[train_df.dataset == dataset]\n",
    "                test_df = test_df.loc[test_df.dataset == dataset]\n",
    "            if FILTER_ON_FAMILIARITY:\n",
    "                train_df = train_df.loc[train_df.confidences_raw == 1]\n",
    "                test_df = test_df.loc[test_df.confidences_raw == 1]\n",
    "\n",
    "            for i, metric in enumerate(METRICS_TO_USE):\n",
    "                # estimate a model for the metric, then make predictions\n",
    "                if dataset in metric:\n",
    "                    continue # don't re-do the internal\n",
    "                elif task == \"intrusions\":\n",
    "                    probs = fit_predict_intrusion(train_df, test_df, metric)\n",
    "                elif task == \"ratings\":\n",
    "                    probs = fit_predict_ratings(train_df, test_df, metric)\n",
    "                fp_rate, pos, fn_rate, neg, agree_rate = false_discovery_rate_sim(\n",
    "                    task, test_df, probs, n_iters=LOCAL_ITERS, alpha=ALPHA, beta=BETA\n",
    "                )\n",
    "                if isinstance(metric, str):\n",
    "                    metric_base = re.search(\"c_npmi_10|c_v|c_uci|u_mass\", metric).group(0)\n",
    "                    reference = metric.replace(f\"{metric_base}_\", \"\") \n",
    "                else:\n",
    "                    metric_base = \"combined\"\n",
    "                    reference = \"full\"\n",
    "                row = {\n",
    "                    \"iter\": iteration,\n",
    "                    \"task\": task,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"metric\": metric_base,\n",
    "                    \"reference\": reference,\n",
    "                    \"false_discoveries\": fp_rate,\n",
    "                    \"discoveries\": pos,\n",
    "                    \"false_omissions\": fn_rate,\n",
    "                    \"omissions\": neg,\n",
    "                    \"agree_rate\": agree_rate, \n",
    "                }\n",
    "                rows.append(row)\n",
    "simulations = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulations.to_csv(\"simulation_results.csv\", index=False)\n",
    "simulations = pd.read_csv(\"simulation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations.loc[(simulations.dataset==\"wikitext\") & (simulations.reference == \"full\"), \"reference\"] = \"wikitext_full\"\n",
    "simulations.loc[(simulations.dataset==\"nytimes\") & (simulations.reference == \"full\"), \"reference\"] = \"nytimes_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_COL = \"false_discoveries\"\n",
    "\n",
    "def ci_lb(x):\n",
    "    return np.quantile(x, 0.025)\n",
    "def ci_ub(x):\n",
    "    return np.quantile(x, 0.975)\n",
    "\n",
    "simulations_grouped = (\n",
    "    simulations.groupby([\"task\", \"dataset\", \"metric\", \"reference\"])[VALUE_COL]\n",
    "               .agg([\"mean\", \"std\", ci_lb, ci_ub])\n",
    "               .reset_index()\n",
    "               .rename(columns={\"mean\": VALUE_COL, \"ci_lb\": \"ci_0.025\", \"ci_ub\": \"ci_0.975\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_TO_KEEP = [\"c_v\", \"c_npmi_10\"]\n",
    "simulations_pivot = (\n",
    "    simulations_grouped.loc[simulations_grouped.metric.isin(METRICS_TO_KEEP)]\n",
    "               .sort_values([\"task\", \"dataset\", \"metric\", \"reference\"])\n",
    "               #.pivot(index=[\"Metric\", \"Reference\"], columns=[\"Task\", \"Dataset\"], values=\"coef\")\n",
    "               .pivot(index=[\"task\", \"dataset\"], columns=[\"metric\", \"reference\"], values=[VALUE_COL, \"std\"])\n",
    ")\n",
    "simulations_pivot = simulations_pivot[[\n",
    "    (v, m, r)\n",
    "    for v in [VALUE_COL, \"std\"]\n",
    "    for m in [\"c_npmi_10\", \"c_v\"]\n",
    "    for r in [\"nytimes_full\", \"wikitext_full\", \"train\", \"val\"] #NB: test excluded\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bold format the significant values\n",
    "for idx, row in simulations_pivot.iterrows():\n",
    "    newrow = []\n",
    "    for i, x in enumerate(row[VALUE_COL]):\n",
    "        if np.isnan(x):\n",
    "            val = \"-\"\n",
    "        else:\n",
    "            std = row[\"std\"][i] * 100\n",
    "            val = f\"${x*100:0.0f}_{{{std:0.0f}}}$\"\n",
    "        newrow.append(val)\n",
    "    simulations_pivot.at[idx, VALUE_COL] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\begin{tabular}{ll|rrrr|rrrr}\n\\toprule\n        &  & \\multicolumn{4}{c}{\\abr{npmi} (10-token window)} & \\multicolumn{4}{c}{$C_v$ (110-token window)} \\\\\n        & Ref. $\\rightarrow$ & \\abr{nyt} & \\abr{wiki} &      Train &        Val & \\abr{nyt} & \\abr{wiki} &      Train &        Val \\\\\n & Train $\\downarrow$ &              &               &            &            &              &               &            &            \\\\\n\\midrule\nIntrusion & \\abr{nyt} &    $23_{22}$ &     $25_{24}$ &  $23_{22}$ &  $26_{22}$ &    $25_{22}$ &     $26_{22}$ &  $24_{21}$ &  $22_{22}$ \\\\\n        & \\abr{wiki} &    $39_{30}$ &     $40_{30}$ &  $38_{29}$ &  $75_{18}$ &    $40_{31}$ &     $40_{30}$ &  $40_{30}$ &  $84_{15}$ \\\\\n        & Both &    $19_{16}$ &     $20_{17}$ &  $18_{15}$ &  $54_{19}$ &    $20_{16}$ &     $22_{16}$ &  $22_{16}$ &  $79_{15}$ \\\\\nRating & \\abr{nyt} &    $43_{22}$ &     $47_{20}$ &  $40_{22}$ &  $30_{19}$ &    $40_{15}$ &     $43_{18}$ &  $38_{15}$ &  $31_{18}$ \\\\\n        & \\abr{wiki} &    $53_{23}$ &     $51_{22}$ &  $43_{25}$ &  $52_{29}$ &    $52_{21}$ &     $49_{22}$ &  $48_{22}$ &  $64_{23}$ \\\\\n        & Both &    $41_{14}$ &     $43_{15}$ &  $34_{15}$ &  $21_{21}$ &    $37_{13}$ &     $38_{14}$ &  $35_{12}$ &  $33_{20}$ \\\\\n\\bottomrule\n\\end{tabular}\n\n"
     ]
    }
   ],
   "source": [
    "# make latex\n",
    "simulations_pivot_values = simulations_pivot[VALUE_COL].loc[[\n",
    "    ('intrusions',  'nytimes'),\n",
    "    ('intrusions', 'wikitext'),\n",
    "    ('intrusions',      'all'),\n",
    "    (   'ratings',  'nytimes'),\n",
    "    (   'ratings', 'wikitext'),\n",
    "    (   'ratings',      'all'),\n",
    "]]\n",
    "latex = simulations_pivot_values.to_latex(escape=False, multicolumn_format='c', column_format=\"ll|rrrr|rrrr\")\n",
    "to_replace_in_latex={\n",
    "    \"c_v\": r\"$C_v$ (110-token window)\",\n",
    "    \"test\": \"Test\",\n",
    "    \"c_npmi_10\": r\"\\abr{npmi} (10-token window)\",\n",
    "    \"nytimes_full\": r\"\\abr{nyt}\",\n",
    "    \"wikitext_full\": r\"\\abr{wiki}\",\n",
    "    \"wikitext\": r\"\\abr{wiki}\",\n",
    "    \"nytimes\": r\"\\abr{nyt}\",\n",
    "    \"all\": \"Both\",\n",
    "    \"full\": \"Full\",\n",
    "    \"train\": \"Train\",\n",
    "    \"val\": \"Val\",\n",
    "    \"ratings\": \"Rating\",\n",
    "    \"intrusions\": \"Intrusion\",\n",
    "    \"metric\": \"\",\n",
    "    \"dataset\": r\"Train $\\downarrow$\",\n",
    "    \"reference\": r\"Ref. $\\rightarrow$\",\n",
    "    \"task\": \"\",\n",
    "}\n",
    "for to_replace, val in to_replace_in_latex.items():\n",
    "    latex = latex.replace(to_replace, val)\n",
    "print(latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}